BURSTING THE FILTER BUBBLE: DEMOCRACY , DESIGN, AND ETHICS 
Proefschrift 
ter verkrijging van de graad van doctor 
aan de Technische Universiteit Delft, 
op gezag van de Rector Magnificus prof. ir. K. C. A. M. Luyben, voorzitter van het College voor Promoties, 
in het openbaar te verdedigen op 
woensdag, 16 September 2015 om 10:00 uur 
door 
Engin BOZDAG˘ 
Master of Science in Technische Informatica 
geboren te Malatya, Turkije.
Dit proefschrift is goedgekeurd door: 
Promotors: 
Prof. dr. M.J. van den Hoven 
Prof. dr. ir. I.R. van de Poel 
Copromotor: 
dr. M.E. Warnier 
Samenstelling promotiecommissie: 
Rector Magnificus, voorzitter 
Prof. dr. M.J. van den Hoven Technische Universiteit Delft, promotor Prof. dr. ir. I.R. van de Poel Technische Universiteit Delft, promotor dr. M.E. Warnier Technische Universiteit Delft, copromotor 
Independent members: 
dr. C. Sandvig Michigan State University, USA 
Prof. dr. M. Binark Hacettepe University, Turkey 
Prof. dr. R. Rogers Universiteit van Amsterdam 
Prof. dr. A. Hanjalic Technische Universiteit Delft 
Prof. dr. ir. M.F.W.H.A. Janssen Technische Universiteit Delft, reservelid 
Printed by: CPI Koninklijke Wöhrmann 
Cover Design: Özgür Taylan Gültekin 
E-mail: engin@bozdag.nl 
WWW: http://www.bozdag.nl 
Copyright © 2015 by Engin Bozda˘g 
All rights reserved. No part of the material protected by this copyright notice may be reproduced or utilized in any form or by any means, electronic or mechanical, includ ing photocopying, recording or by any information storage and retrieval system, without written permission of the author. 
An electronic version of this dissertation is available at 
http://repository.tudelft.nl/.
PREFACE 
For Philip Serracino Inglott, 
For his passion and dedication to Information Ethics 
Rest in Peace. 
Engin Bozda˘g 
Delft, August 24, 2015 
iii
CONTENTS 
1 Introduction 1 1.1 Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 1.2 Problem Statement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 1.3 Research Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 1.4 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 
2 Bias in Algorithmic Filtering and Personalization 13 2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 2.2 Information overload and the rise of the filters . . . . . . . . . . . . . . . 15 2.3 Personalization – a technical overview . . . . . . . . . . . . . . . . . . . 17 2.4 A model of Filtering for Online Web Services . . . . . . . . . . . . . . . . 20 
2.4.1 Source Selection Algorithm . . . . . . . . . . . . . . . . . . . . . 21 2.4.2 Information Selection and Prioritization Algorithm . . . . . . . . . 22 2.4.3 Human operator . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 2.4.4 Personalization Algorithm . . . . . . . . . . . . . . . . . . . . . . 25 
2.5 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 2.5.1 Implications for an ethical analysis . . . . . . . . . . . . . . . . . 29 2.5.2 Implications for design. . . . . . . . . . . . . . . . . . . . . . . . 32 2.5.3 Implications for the design of social filtering . . . . . . . . . . . . . 32 2.5.4 Implications for social network analysis . . . . . . . . . . . . . . . 34 2.6 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 
3 Viewpoint Diversity in Online Social Networks - An Empirical Analysis 37 3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 3.2 Empirical Studies of Information Diversity in Social Media . . . . . . . . . 39 3.3 Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 
3.3.1 Information Diversity . . . . . . . . . . . . . . . . . . . . . . . . 40 3.3.2 Dimension of Information Diversity . . . . . . . . . . . . . . . . . 41 3.3.3 Minorities and Openness . . . . . . . . . . . . . . . . . . . . . . 42 
3.4 Polarization in the Netherlands and Turkey . . . . . . . . . . . . . . . . . 42 3.4.1 the Netherlands . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 3.4.2 Turkey . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 3.4.3 Conclusion. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 
3.5 Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 3.5.1 Data collection. . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 3.5.2 Research questions . . . . . . . . . . . . . . . . . . . . . . . . . 45 3.5.3 Entropy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 3.5.4 Translating Research Questions into Metrics . . . . . . . . . . . . . 46 
v
vi CONTENTS 
3.6 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 3.6.1 Distribution of seed users and their followers . . . . . . . . . . . . 48 3.6.2 Seed User Bias . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 3.6.3 Source and Output Diversity . . . . . . . . . . . . . . . . . . . . . 49 3.6.4 Minority Access . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 3.6.5 Input Output Correlation . . . . . . . . . . . . . . . . . . . . . . 50 
3.7 Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 3.8 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 
4 Democracy, Filter Bubble and Design 55 4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55 4.2 Democracy: different theories, different benchmarks . . . . . . . . . . . . 56 
4.2.1 Liberal View of Democracy. . . . . . . . . . . . . . . . . . . . . . 57 4.2.2 Deliberative Democracy . . . . . . . . . . . . . . . . . . . . . . . 58 4.2.3 Republicanism and Contestatory Democracy . . . . . . . . . . . . 59 4.2.4 Agonism / Inclusive Political Communication . . . . . . . . . . . . 61 4.2.5 Conclusion. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62 
4.3 Software Design to Combat Filter Bubbles . . . . . . . . . . . . . . . . . 65 4.3.1 Liberal / User Autonomy Enhancing . . . . . . . . . . . . . . . . . 65 4.3.2 Deliberative / Enhancing Epistemic Quality of Information . . . . . 68 
4.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71 4.5 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77 
5 Ethics of Online Social Experiments 79 5.1 The Facebook Emotional Contagion Experiment . . . . . . . . . . . . . . 79 5.2 The Unclear Ethical Terrain of Online Social Experiments. . . . . . . . . . 80 5.3 Arguments For and Against Online Social Experiments . . . . . . . . . . . 81 
5.3.1 Benefits of Online Experiments for the Individual . . . . . . . . . . 82 5.3.2 Informed Consent and Its Many Interpretations . . . . . . . . . . . 83 5.3.3 The Ubiquity of Online Social Experiments . . . . . . . . . . . . . 84 5.3.4 Different Perceptions of Risk in Online Experiments . . . . . . . . . 85 5.3.5 Benefits of Online Experimentation for the Society . . . . . . . . . 85 5.3.6 The Unavoidability of Online Experiments. . . . . . . . . . . . . . 86 5.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87 
6 Conclusion 91 6.1 Answers to the Research Questions and Revisiting the Objectives of the Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92 6.2 Limitations of the Study. . . . . . . . . . . . . . . . . . . . . . . . . . . 94 6.3 Future Research. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95 6.3.1 Viewpoint Diversity Research . . . . . . . . . . . . . . . . . . . . 95 6.3.2 Transparency . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97 
Acknowledgements 101 Summary 103 Samenvatting 107
CONTENTS vii 
Curriculum Vitæ 111 6.4 Scientific Publications . . . . . . . . . . . . . . . . . . . . . . . . . . . 111 6.4.1 Journal Articles. . . . . . . . . . . . . . . . . . . . . . . . . . . . 111 6.4.2 Conference Proceedings . . . . . . . . . . . . . . . . . . . . . . . 111 6.5 Professional/Popular Publications . . . . . . . . . . . . . . . . . . . . . 112 
References 113
1 
INTRODUCTION 
The peculiar evil of silencing the expression of an opinion is, that it is robbing the human race; posterity as well as the existing generation; those who dissent from the opinion, still more than those who hold it. If the opinion is right, they are deprived of the opportunity of exchanging error for truth: if wrong, they lose, what is almost as great a benefit, the clearer perception and livelier impression of truth, produced by its collision with error. 
John Stuart Mill, philosopher, 1859 
Should we go to war? How high should the taxes be? What happened at Gezi Park in Istanbul? These are all political questions where different sources would give a differ ent answer. With the advent of social media as an important source of news and opin ions [183, 230], some activists and scholars have started worrying that the Internet could lead to online segregation and may increase radicalism and extremism in society, due to receiving biased and one-sided news and opinions. Critics have pointed out the dan gers of group forming among like-minded in Internet. Recently, online platforms such as Facebook and Google have been criticized, because with their opaque personaliza tion algorithms they show users viewpoints that they already agree with, hence lead ing to information silos, or so-called filter bubbles. The reason why filter bubbles have been criticized differs. Some argue that the opaque algorithms used by online platforms make decisions on behalf of the user, coercing him and making him unaware of available choices. Others argue that biases caused by algorithms and human beings themselves might diminish viewpoint diversity, decrease respect toward one another or allow op pressors to prevail due to a lack of information to the citizens, which will prevent them reaching the truth or only one side of the truth. Viewpoint diversity has long been viewed as an essential component of strong democratic societies [82, 150]. Yet others, including Google and Facebook, have argued that the effects of personalization have been exagger ated [26, 249, 352]. The immediate question that comes to mind is whether filter bub bles really exist. However, to answer this question empirically and properly, one must first define what filter bubble is. Do online platforms really have biases that may cause bubbles? Do people themselves construct self-reinforcing filters because they already 
1
2 1. INTRODUCTION 
have divergent beliefs? This interdisciplinary thesis aims to answer such questions by 1 
studying filter bubble phenomenon and the relevant value viewpoint diversity 1) at the conceptual level by using theories from by political philosophy, media and communica tion and ethics 2) at the empirical level by analyzing filter bubble by extending viewpoint diversity analytics metrics 3) at the technical level, by analyzing tools that are designed to combat filter bubbles. 
1.1. BACKGROUND 
According to a study in the US, nearly half (48%) of the panelists say they accessed news about politics and government on Facebook alone in the past week [230]. More than twice as many Americans are following political candidates on social media in 2014 as was the case during the 2010 midterms, indicating that these platforms are playing an increasingly prominent role in how voters receive political information and follow elec tion news [230]. The results of another study show that among the so-called X genera tion (born between 1966 and 1980), Internet news audience jumped from 29 percent to 49 percent between 2004 and 2012 [183]. It now matches turning to TV for news, which also declined by 20 percentage points over this period. Similar patterns are apparent among Millennials (born between 1980 and 1995), but they are more extreme. More of those now turn to the Internet for news than to TV (43% versus 35%). A more recent study indicates that 86% of the Millennials usually turn to social media to receive diverse opin ions, more than any other media [13]. It is also reported that social media users increas ingly express their opinions about the world, national and local news. Between 2010 and 2012, the traffic to news sites from various social media platforms grew 57% [262]. Media companies are starting to get more and more dependent on Facebook for traffic. Vox, for instance, gets 40 percent of visits through Facebook. Other leading news organizations get around a quarter of site visits from the social networking platform [194]. The New York Times and BuzzFeed confirmed plans to begin hosting their work directly on Face book using a feature called “Instant Articles” [54, 225]. Other studies indicate that on line information intermediaries such as Facebook and Twitter are prominent platforms where users follow news about governments and politics [230]. These platforms are thus slowly replacing traditional media channels thereby partly becoming the gatekeepers of our society [45, 83, 294] 
These popular platforms are often seen as revolutionary participatory information production and consumption infrastructures that help to overcome barriers to informa tion. This is due to Internet’s high reach [123], diversity and interactivity [286]. According to this optimistic view, news and political information will not be produced and dissem inated by only a small number of elites. It will disrupt the power of the elites and will allow a more inclusive discussion, because everyone can voice his or her opinion. The supposition is that there are no or fewer gatekeepers, thus more and diverse viewpoints will be represented online than in offline media [359]. Minorities, the disadvantaged and others who could not utter their voices will be able to do so in new media [53]. Many scholars and journalists describe the online media landscape created by search engines and social media platforms as the “age of plenty”, with “an almost infinite choice and unparalleled pluralization of voices that have access to the public sphere”[66, 126, 174]. “Because of its horizontal, open, and user-friendly nature, the Internet allows for easy
1.1. BACKGROUND 
3 
access to, and thus greater participation, in the public sphere” [47, p.144]. 
1 
These inclusive platforms then promote equal access to diverging preferences and opinions in society and allow people to be exposed to diverse viewpoints. Some further claim that, since the control and power of the elites in traditional media do not work in new media, journalists can communicate virtually anything to anyone [196]. Social network sites such as Twitter are seen as platforms where anyone can affect the news creation process [159]. In short, in this optimistic view, online intermediaries allow any one to be able to contribute to the public debate and reach a large public to be able to read anything published by anyone, without any barriers. In the end, Internet and particularly social media will have a positive impact in online participation and democ ratization due to the personal and interactive nature of the online platforms. 
However, such an optimistic view of the Internet as an endless information platform with a diverse set of viewpoints has received many criticisms. Cyber-pessimists argue that online platforms are not that different from traditional media, as the same small set of elites still determine how the political information is obtained, filtered and presented [333]. Redden and Witsche [2009] argue that the use of the Internet for political commu nication is minor. This is because consumerism, entertainment, non-political network ing and online chat dominates the online sphere, not every political item gets attention and political items do not spread widely. This claim is supported with Facebook’s latest study where the researchers found out that only 7% of the content users click are “hard news” [24]. Further, on platforms such as Facebook the degree of involvement might dif fer per story and even if users do participate in political communication, they only get involved in the last phase of ‘traditional’ news production process by interpreting texts and commenting on them, and not at the decision-making stages of news production [274]. Witschge’s [2008] analysis on Dutch fora shows that even though an alternative or radical voice can be expressed online, it is not successful in opening up a dialogue. Instead, the participants were trying to find ways to exclude it. Others emphasize the important role of opinion leaders and those in higher social hierarchy in shaping others’ perceptions during a conflict and argue that not everyone has the same right to be heard [206]. Papacharissi [2002] argues that online discourse often ends in miscommunica tion and therefore cannot directly enhance democracy. Hindman [2008] and Sunstein [2007] argue that Internet is not a public forum due to the ease of only reading and link ing to like-minded resources and deliberating among like-minded users without hearing contrary views. Due to this homophily, social fragmentation and group polarization will follow which will lead users to even more extreme positions. Others have provided evi dence to support this claim [3, 72, 155, 314]. In short, cyber-pessimists argue that social media can lead users to deliberate among their own groups. 
While many scholars have argued that users may create their own filters and deliber ate among like-minded groups, Pariser [2011] focused on opaque and hidden filters used by the online platforms themselves. In his book “The Filter Bubble: What the Internet Is Hiding from You”, Pariser [2011] mentions that online platforms personalize the infor mation per user, depending on user’s previous interaction history with the system, his preferences and his contacts in social media. The consequence of such social filtering is that results in a search engine for the same query differ per user and two people with the same friends in a social network might see different updates and information. This,
4 1. INTRODUCTION 
Pariser argues, might create a monoculture, in which users get trapped in their “filter 1 
bubble". Pariser [2011] writes, “personalization filters serve up a kind of invisible auto propaganda, indoctrinating us with our own ideas, amplifying our desire for things that are familiar and leaving us oblivious to the dangers lurking in the dark territory of the unknown.” [p. 13] 
Pariser points out that such “social filtering” may ultimately undermine civic dis course by confirming our existing ideas and limiting our exposure to opposing view points. These algorithms can cause citizens to be ill-informed about current events and the citizens may have increasingly “idiosyncratic perceptions” about the importance of current events and political issues. This might occur because online services are trying to improve accuracy at the expense of serendipity and diversity, often in order to in crease user retention. Pariser argues that even if users wanted to diversify their network explicitly, information intermediaries silently filter out what they assume the user does not want to see, hiding information posted by opposite end of the political spectrum. Pariser believes that the algorithmic gatekeepers need to show us things that are not only easy to consume but also things that are challenging, important and uncomfortable and present competing points of view. 
After Pariser’s book led to lengthy public discussions and academic debates, researchers wanted to find out whether filter bubbles really exist. Some of the studies support the filter bubble theory. An et al. [2014] found indications of partisan sharing on Facebook. Eslami [2015] found that majority of the studied Facebook users were not aware of al gorithmic filters and were very upset when they found out close friends and family were not shown in their feeds. Pentina and Tarafdar [2014] found out that especially those who have a small network on Facebook are vulnerable to the filter bubble effect. Nikolov et al. [2015] studied 4 years of communication data in a university network and found out that social media exposes the community to a narrower range of information sources. Hoff man et al. [2014]’s study shows that users avoid news sources that belong to the opposite end of the political spectrum. Conover et al. [68] found that retweet network in American Twittersphere is highly polarized, while the mention network is not. Kim and Park [2012] provide evidence of polarization in Korean Twittersphere. Liu and Weber [2014] argues that Twitter is not an “idealized space for democratic, rational cross-ideological debate”, because individuals who do not have many followers does not interact much with their ‘ideological-foes’, and tend to attack opinion leaders with many followers if they choose to interact with them. Further, duration of discussions among like-minded users last longer and more than 40% cross-ideological tweets were disagreements [205]. Yom-Tov et al. [2013] studied reading pattern of Bing search engine users and observed that ma jority of the Republicans and democrats click on sources that belong to their viewpoint, and rarely click on sources that belong to the other side. Gruzd and Roy [2014] found that, in Canadian Twittersphere, people do tend to cluster around shared political views and political polarization exists. Further, 40% of the messages between ideologically dis tant parties were hostile or negative [145]. Colleoni et al. [67] found out that bubbles ex ist in the US Twittersphere, however structures of political homophily differ strongly be tween Democrats and Republicans, and between general users and those who follow the political parties. Barbera and Rivero [29] studied American and Spanish Twittersphere and found out that users participating in the political discussion were mostly men, liv-
1.1. BACKGROUND 
5 
ing in urban areas, and with strong ideological preferences. They also found out that 1 
users who follow political parties directly are more active in the political discussion than the rest [29]. Freelon et al. [110] studied Syrian Twittersphere and observed high frag mentation among different communities. Smith et al [2014] tracked one hashtag related to America’s budget conflict over two days. They found “two big and dense groups that have little connection between them” [299]. Grevet et al. [138] performed a survey with 103 politically engaged Facebook users and found out that weak ties (contacts who are not very close to the user and not like-minded) offer the most diversity, yet users engage less with with them and they can break under pressure from contentious discussions. Gromping [143] analyzed Facebook posts in 2014 Thai general election and found that partisan social media users hardly engaged with opposing viewpoints at all. Hahn [151] studied South Korean Twitter users and revealed polarization and the reinforcement of existing political divisions in society. 
However, others, including the online platforms themselves, disagree with Pariser. Google claims that they actually limit personalization and promote variety for certain topics [353]. Facebook’s first study argued that people get exposed to novel content through their weak links and therefore bubbles do not exist [26]. More recently Face book studied filter bubble for self-reported liberals and conservatives and founded out that the News Feed does ideologically filter what we see [24]. In its press outreach, Face book has emphasized that “individual choice” matters more than algorithms do. Yet, Facebook argues that this effect is modest in comparison to the choices people make that filter information, including who their friends are and what they choose to read given the curation. Barbera [28] studied the ideological positions of users in Germany, Spain and the United States and found out that social media users receive information from a set of diverse sources, thanks to weak ties. Others argue that bubbles already oc curred in traditional media as local newspapers reflected a narrow consensus and with the infinite choices on the Web, people will actually choose to consume a more diverse set of content [353]. O’Hara et al. [2015] argue that the evidence for bubbles is not strong enough for regulation and even if bubbles exist, users can escape them. Since users can live in looser and multiple networks (often thanks to social media), they have flexibility, choice and exposure to heterogeneous points of view. Weinberger [352] argues that; 1) the Internet is too young to make conclusions about filter bubbles; 2) the empirical re search that exists is very difficult to interpret; 3) fragmentation occurs in traditional me dia and in offline world; 4) democracy needs bubbles so that people in basic agreement can build relationships and be active in political movements. Beam and Kosicki [35] in vestigated the impact of personalized news web portals on political bias and found out the average news viewer seems to favor news that does not have bias towards a particular perspective. 
Since the term filter bubble has been uttered in 2011 by Pariser, it has received a broader meaning. While personalization bias is a recent phenomenon, algorithmic bias is not. Inclusivity issues of search engines, lack of transparency and other factors that cause bias have been discussed before [135, 168, 334]. As Chapter 2 will show, online platforms suffer from algorithmic and human biases. However, online platforms are not the only source of bias. Psychology literature, particularly “selective exposure theory”, studies biases in human information seeking. Issues studied under selective exposure
6 1. INTRODUCTION 
include cognitive dissonance (psychological discomfort arising from one’s partisan at 1 
tachment and information challenging this attachment)[104], confirmation bias (seek ing information for a fast conclusion or stopping seeking new information when a con clusion is reached) [288], avoiding information overload (by reducing sources and ideas, processing information faster or avoiding information seeking all together) [262, 306], avoiding challenging information (because it requires more resources to process[306], and bias in determining quality of information [105]. Further, Sunstein [2007] argued that, due to the availability of manual filters on the Internet and the option to commu nicate only with like-minded others, group polarization will arise and people will end in more extreme positions. Sunstein used the term “echo chambers” to conceptualize this group polarization [314]. Filter bubble is now also used to describe this behavior as well. [110, 138, 199, 202]. The term now not only encompasses opaque automatic cyberbalka nization imposed on users by the algorithms of the online platforms as emphasized by Pariser, but it also includes other non-automatic voluntary selective exposure and bi ased information seeking and group polarization. This gives us the situation depicted in Figure 1.2. Biases in design and operation of online intermediaries and biases caused by selective exposure (thanks to manual filters) lead to biased online platforms that we now define as “the filter bubble". 
Filter bubbles are mainly criticized due to their two important consequences. First, opaque filters that are imposed upon the user, diminishes their autonomy [242, 256]. Since it is not clear that filters are there at the first place, the user might not know that the information stream is filtered after all. Further, even if the existence of the filters were known, since every user might get a different output of the system, it will be different for the user to compare what he is missing. A non-filtered version often is not available. A daily user of social networking platforms describes this as follows: 
Never mind how much data you have about me. No, sorry, systems should 
not be making that choice for people. At least, people should be given the opportunity to opt-in or opt-out of that model, which in this case it’s just not happening. And I am finding that incredibly frustrating and perhaps some what disturbing as well for that matter. Why? Mainly, of course, because of that filter bubble. I would want to be the person in charge of what I get ex posed to, how I would want to get exposed to, and, most importantly, how I would want to consume that content shared across. And let it be down to me to decide if I would want to mitigate, or not, the fire hose effect of content I get exposed to 
Secondly, it has been argued that, due to algorithms being designed for accuracy and short-term user satisfaction information diversity is lost [256]. Following Napoli’s framework of media diversity [243], this leads to a decrease in viewpoint (idea) diversity, topic (content) diversity, source diversity and in the end exposure diversity (See Figure 1.1). As we will discuss in Chapter 4, a decrease in diversity of ideas, opinions and per spectives is undesired for almost all models of democracy. Following Rawls’ concept of ‘primary goods’ [272], goods that are supposedly useful (or at least not harmful) to any one, irrespective of their conception of the good, van den Hoven and Rooksby [2008] argued that information should be accepted as a primary good. Information online is
1.1. BACKGROUND 
7 
vital for people to plan their lives rationally and to participate adequately in the com 1 
mon life of their societies [344]. Thus, filter bubbles affect having access to information, which in turn affects the worth of liberty felt by an individual. Contrary to earlier stages of the Internet-era, when the problem information access boiled down to having access to hardware, nowadays the problem of access to information concerns the ability of in tentionally finding the right information, or unintentionally stumbling on upon relevant information [46]. 
Figure 1.1: Filter bubble 
  

The phenomenon of filter bubble has received much attention in news, academia and in the industry. For instance, due to the critique on social media, Facebook per formed empirical studies to investigate whether bubbles occur [24, 26]. However, as we will discuss in Chapter 2 and 4, these studies use various and different notions of democ racy (if defined at all) to analyze filter bubble. Filter bubble is a complex phenomenon that requires a good analysis of the underlying concepts, such as equal access, inclu siveness and autonomy. Existing theoretical work, for instance from computer ethics or media communication often lack scientific empirical backing. Further, the existing empirical work testing filter bubble in social media define filter bubble and the relevant value diversity rather implicitly. This leads to the conceptualization of the filter bubble as not “hearing the other side" [40, 228, 240] with two dominant sides or not being “exposed to the mainstream”[249]. However, according to media diversity literature, viewpoint di versity is not only about two main sides hearing each other or avoiding extremism by be ing exposed to the mainstream, but also about the radical voices and non-mainstream opinions to reach a larger public effectively. Therefore, different operationalization of viewpoint diversity can lead to different results in empirical work. Further, all work on filter bubble criticize the algorithms that the platforms operate on, however they do not delve into details of factors that causes the bubbles. Do the factors that affect traditional media also affect the new media? How do humans influence the online platforms next to algorithms? Are there other influences? What is the cause of the filter bubble? In this
8 1. INTRODUCTION 
thesis, we will aim to answer such questions. 
1 
1.2. PROBLEM STATEMENT 
The main aim of this thesis is to reduce the knowledge gap on filter bubble between different disciplines and between the theory and practice. The main research question of this thesis can be formulated as follows: 
RQ: How can we design for viewpoint diversity, so that filter bubbles are avoided? To answer this research question, the following subquestions are defined: 
RQ1: What is filter bubble and which factors lead to it? 
While many works, including Pariser’s (2011) book point out the dangers of online filters, they do not delve into details that may cause these issues. Further, Pariser’s work does not refer to a rich literature on gatekeeping and search engine bias. Chapter 2 aims to contribute to the existing literature on gatekeeping theory and search engine bias by ex tending the traditional gatekeeping theory with algorithmic gatekeeping performed by online services including personalization. It shows that factors affecting traditional me dia, such as advertiser bias, personal judgments, or organizational bias are also present in online platforms. Further, it shows that bias is not only invoked by algorithms, but also by human operators of those platforms. It shows that switching from human editing to algorithmic gatekeeping does not remove all human biases. 
RQ2: Can we conceptualize and operationalize viewpoint diversity? 
There are many empirical work that studied source diversity [244], polarization [15, 24, 67, 68, 110, 145, 178, 205], media bias [14, 280, 351], user bias in search engines [368], URL novelty [26], and weak-links [24, 138]. These studies present conflicting results. Some claim that bubbles do not exist [24, 28], while others claim that they do [143, 151]. However, according to media and communication literature, viewpoint diversity is not only measured by the number of available viewpoints or the interaction between major political fractions, but also by the possibility of the minorities and other disadvantaged to reach the larger public. Chapter 3 of this thesis first introduces different dimensions of the highly complex value viewpoint diversity using political theory and provides an overview of the metrics used in the literature of viewpoint diversity analysis. Later, it also proposes new metrics using media ethics theory and provide a framework to an alyze viewpoint diversity in Twitter for different political cultures. Finally, it presents the results for a case study on minorities that is performed for Turkish and Dutch Twit ter users and shows that minorities cannot reach a large percentage of Turkish Twitter users. With the last of these contributions, using theory from communication scholars and philosophers, this chapter shows how minority access is missing from the typical dimensions of viewpoint diversity studied by computer scientists and the impact it has on viewpoint diversity analysis. 
RQ3: Which issues does filter bubble cause for different models of democracy and what can design do to solve them?
1.3. RESEARCH METHODOLOGY 
9 
Chapter 4 tries to answer these questions by studying different democracy models from 1 
political philosophy and shows how each one of them criticizes a different consequence of the filter bubble. This chapter summarizes most important norms of different democ racy models in relation to viewpoint diversity. It analyzes various software tools and de signs that are developed to fight filter bubbles. It shows that the aims and goals of these tools can differ radically, depending on the designer’s understanding what filter bubble is and why it is an undesirable development. 
RQ4: What are the ethical issues associated with those solutions? 
Online platforms are conducting large scale experiments in order to combat filter bub bles [368], while some argue that those experiments themselves may cause bubbles [220]. In 2014, a controversy has occurred surrounding the so-called Facebook emotional con tagion study [185]. In this study, Facebook wanted to test the assumption that basic emotions, positive and negative, are contagious, that is, that they spread from person to person by exposure. To do this, they have adjusted the News Feed of hunders of thousand of users to randomly filter out specific posts with positive and negative emotion words to which they would normally have been exposed. The article provoked some very strong reactions both in the international news media and among scholars. Chapter 5 analyzes the arguments of two parties (data science advocates and data science critics) through a qualitative discourse analysis. Since similar studies are performed to fight the filter bubble by certain platforms, it is very important to discuss the relevant ethical values, including informed consent, transparency and autonomy. 
1.3. RESEARCH METHODOLOGY 
To answer the research questions in the previous section, we use Value Sensitive De sign (VSD) methodology. VSD is “a theoretically grounded approach to the design of technology that accounts for human values in a principled and comprehensive manner throughout the design process" [116]. The word “value” here is defined rather broadly: “what a person or group of people consider important in life” [116]. A key feature of VSD is a tripartite methodology, consisting of iteratively applied conceptual, empirical, and technical investigations. Conceptual investigations include discovering the relevant val ues that can inspire or inform the design of a tool by using a list of important human values as a starting point (e.g. privacy, autonomy) [108]. This first step also includes us ing theories from other disciplines (e.g. philosophy, media studies, etc.) to conceptualize specific values and clarify any issues that can arise due to different interpretations of the same value by different disciplines or due to value conflicts (e.g., privacy vs security). Empirical investigations include the entire range of qualitative and quantitative meth ods used in social science research. This can include observations, interviews, surveys, or measurement of user behavior. Technical investigations focus on how existing tech nological properties can support or hinder human values. Technical investigations also involve translating the identified values by operationalizing, embodying, implementing and expressing them in system design [108]. 
Since years, a number of critiques and suggestions towards VSD have also been pub lished. For instance VSD takes the position that certain values are universally held and
10 1. INTRODUCTION 
how they exactly play out can differ per culture, which is seen as problematic by many 1 
[10, 43, 73]. Others critique VSD’s ethically neutral stance, leaving unclear what values and which theories it includes, thus making value conflicts unresolvable [9, 214]. This has the danger of committing naturalistic fallacy by taking empirical findings as given, not distinguishing mere preferences from moral values and implementing them into de sign directly without performing a normative analysis [214]. Borning and Mueller [2012] argue that a VSD researcher’s own culture and assumptions may limit the qualities of authority and impartiality, as he might have biases due to his background, experiences and relation to the participants in the study. Similarly, others argue that VSD fails to address the use of deliberative methods and tools to promote joint reflection and stake holder participation during the design process [43, 267]. Further, it has also been pointed out that the values listed by the founders of VSD might be too simplistic, as they stem from a particular culture and viewpoint (Western, upper class academics) and hence they should be contextualized and explicitly stated [43, 76]. Le Dantec et al [2009] argue that having a list of values may blind the researcher to values that fall outside that list. 
While VSD has its weaknesses, it has been an important part of computer human in teraction field under computer science and lead to many influential findings and analy ses [73, 75, 112, 115, 117, 118, 227, 229, 293, 362]. In this thesis, VSD is used, because the complex issues arising from the filter bubble require conceptual, empirical and technical investigations. First of all, there is a conceptual unclarity about the problem. Many peo ple use the same word (filter bubble) to address the issue or devise tools to get rid of it, while they actually talk about different issues and even if they talk about the same issue (information diversity) they have different conceptions of this value. Second, some em pirical studies were performed to find out if bubbles exist. However, since those studies did not have a conceptual investigation of this complex problem, the findings represent one particular interpretation of the filter bubble. This conceptual confusion has man ifested itself in technical artifacts that all claim to “combat filter bubbles”, yet do very different things, or solutions that are seen as unacceptable by some. 
As critics have argued, starting with the list of values VSD has provided and then see which of these values are implicated in filter bubble would be the wrong approach. Instead, we have studied the publications, news and opinions on this topic and then curated a list of values [46]. From this list we have the following values identified: view point diversity, freedom from bias, privacy, informed consent, autonomy, transparency and identity. In this thesis, we will focus on viewpoint diversity and freedom from bias, while also discussing implications for transparency. 
Applying VSD gives us the steps depicted in Figure 1.2. Chapter 2 belongs to concep tual and empirical studies, because it analyzes the concept filtering, bias and gatekeep ing using theories from media and communication studies. It also uses observations to identify factors that cause bias. Chapter 3 belongs to conceptual and empirical in vestigations as it uses theories from media and communication studies on viewpoint diversity and employs quantitative methods to measure user behavior in social media. Chapter 4 belongs to conceptual and technical studies, as it analyzes norms of democ racy using theories from political philosophy and it studies different technical solutions that are developed to combat filter bubbles. Chapter 5 uses theories from various fields to address the value transparency and informed consent and analyzes arguments from
1.4. CONTRIBUTIONS 
11 
different users. Therefore it belongs to the conceptual investigations. 
1 
Figure 1.2: How Value Sensitive Design is applied in this thesis 
  

1.4. CONTRIBUTIONS 
This thesis contributes to the literature by doing the following: 
Chapter 2 is, to our knowledge, the first work that captures various factors that can lead to algorithmic and operational biases in online services, including personalization. Chapter 3 is, to our knowledge, one of the first works that studies filter bubbles em pirically and the first work that operationalizes the concept “equal access” and the first work that compares input and output diversity. It shows the results of an empirical study to show bubbles may also happen due to user’s own grouping habits. It compares differ ent political cultures and defines different metrics. It shows that minorities may become invisible in Twitter for certain political cultures. 
Chapter 4, is, to our knowledge, the first work that analyzes the concept using differ ent democracy models. It analyzes how the design attempts to fight filter bubbles differ fundamentally and how these designs can cause other ethical issues. 
Chapter 5, is, to our knowledge, the first work that studies various arguments that are used in discussing ethical issues of experiments carried by online services. Filter bubbles
12 1. INTRODUCTION 
are either caused by those experiments, or are used to fight them.
1 
2 
BIAS IN ALGORITHMIC FILTERING AND PERSONALIZATION 
A squirrel dying in your front yard may be more relevant to your interests right now than people dying in Africa. 
Mark Zuckerberg, CEO of Facebook 
This chapter has been published in Ethics and Information Technology [45] 
13
14 2. BIAS IN ALGORITHMIC FILTERING AND PERSONALIZATION 
2.1. INTRODUCTION 
Information load is a growing problem in today’s digitalized world. As the networked media environment increasingly permeates private and public life, users create their own enormous trails of data by for instance communicating, buying, sharing or search 2 
ing. The rapid and extensive travelling of news, information and commentary makes it very difficult for an average user to select the relevant information. This creates serious risk to everything from personal and financial health to vital information that is needed for fundamental democratic processes. In order to deal with the increasing amounts of (social) information produced on the web, information intermediaries such as Facebook and Google started to introduce personalization features: algorithms that tailor infor mation based on what the user needs, wants and who he knows on the social web. The consequence of such personalization is that results in a search engine differ per user and two people with the same friends in a social network might see different updates and information, based on their past interaction with the system. This might create a monoculture, in which users get trapped in their “filter bubble” or “echo chambers” [256, 311, 313]. Social media platforms, search and recommendation engines affect what a daily user sees and does not see. As knowledge, commerce, politics and communica tion move online, these information intermediaries are becoming emergent gatekeepers of our society, a role which once was limited to the journalists of the traditional media. 
The gatekeeping process is studied extensively by multiple disciplines, including me dia studies, sociology and management. Gatekeeping theory addresses traditional me dia bias: how certain events are being treated more newsworthy than others and how in stitutions or influential individuals determine which information passes to the receivers [298]. Gatekeeping theory does address the rising power of online information interme diaries, but it focuses on two things: a) the increasing role of the audience in which users can determine what is newsworthy through social networks b) the changing role of the journalist, from a gatekeeper to a gatewatcher [48, 294]. The existing theory often con siders the online information intermediaries themselves as neutral or treats a web ser vice only as an algorithm, operating without human bias [49, 159, 197]. Because these information intermediaries automate their core operations, often, mistakenly, they are treated as objective and credible. Machines, not humans, appear to make the crucial decisions, creating the impression the algorithms avoid selection and description biases inherent in any human-edited media. 
Several authors have shown that computer systems can also contain biases. Fried man and Nissenbaum [1996] show that software can systematically and unfairly discrim inate against certain individuals or groups of individuals in favor of others. Bias can manifest itself in a computer system in different ways; pre-existing bias in society can affect the system design, technical bias can occur due to technical limitations, emergent bias can arise sometime after software implementation is completed and released [113]. Several authors have shown how search engines can contain technical biases, especially in coverage, indexing and ranking [86, 234, 334, 348, 360]. However, these works are only focusing on the popularity bias. As we will show, many other factors can cause bias in online services. 
In this paper we show that online services that process (social) data are not merely algorithms; they are complex systems composed of human operators and technology.
2.2. INFORMATION OVERLOAD AND THE RISE OF THE FILTERS 
15 
Contrary to popular belief, humans do not only take part in developing them, but they also affect the way they work once implemented. Most of the factors that cause human bias in traditional media still play a role in online social media. Finally, even though personalization is seen as a solution by some to prevent technical biases that exist in non-personalized online services [129], we show that personalization not only intro 
2 
duces new biases, but it also does not eliminate all of the existing ones. Others have already pointed to the dangers of implicit and explicit personalization in online services and traditional media [176, 256, 311, 346]. However, they do not identify the potential sources of bias, processes and factors that might cause particular biases. They also do not connect this debate to existing literature in gatekeeping and search engine bias. Our descriptive model of algorithmic gatekeeping aims to achieve this. As Goldman [2011] has recently written about search engine bias: “competitive jostling has overtaken much of the discussion. It has become almost impossible to distinguish legitimate discourse from economic rent-seeking”. This overview of bias will hopefully serve as a reference point and contribute to further rational discussion. 
Friedman and Nissenbaum [1996] argue that technical bias places the demand on a designer to look beyond the features internal to a system and envision it in a context of use. Minimizing bias asks designers to envision not only a system’s intended situation of use, but to account for increasingly diverse social contexts of use. Designers should then reasonably anticipate probable contexts of use and design for these. If it is not possible to design for extended contexts of use, designers should attempt to articulate constraints on the appropriate contexts of a system’s use. We believe that our detailed model will help designers and policy makers to anticipate these probable contexts of use and for mulate scenarios where bias can occur. The paper is structured as follows: In Section 2, we give background information to the problem. In Section 3, we give a summary of personalization and how it poses unique problems. In Section 4, we introduce a model of algorithmic and human filtering for online web services including personalization. In Section 5, we discuss implications for ethical analysis, social network analysis and de sign. Section 6 concludes this paper and lists several questions for future research. 
2.2. INFORMATION OVERLOAD AND THE RISE OF THE FILTERS According to Cisco, in 2015, the amount of consumer generated data on the Internet will be four times as large as it was in 2010 [62]. McKinkey’s research shows that “big data” is a growing torrent. In 2010, 30 billion pieces of content were shared every month with 5 billion mobile phones contributing to it [217]. An IBM study reports that every two days we create as much digital data as all the data (digital or non-digital) that was created before 2003 and 90% of the information in the world today has been created in the last two years alone [166]. In online (social) services, users actively contribute explicit data such as information about themselves, their friends, or about the items they purchased. These data go far beyond the click-and-search data that characterized the first decade of the web. Today, thanks to the advent of cloud computing, users can outsource their computing needs to third parties and online services can offer software as a service by storing and processing data cheaply. This shifts the online world to a model of collabo ration and continuous data creation, creating so-called “big data”, data which cannot be processed and stored in traditional computing models [217].
16 2. BIAS IN ALGORITHMIC FILTERING AND PERSONALIZATION 
Even though the amount of generated data on the social web has increased exponen tially, our capabilities for absorbing of this information have not increased. Because the mind’s information processing capacity is biologically limited (for example, we possess neither infinite nor photographic memory), we get the feeling of being overwhelmed by the number of choices and end up with “bounded rationality” [160]. Researchers across 
2 
various disciplines have found that the performance (i.e., the quality of decisions or rea soning in general) of an individual correlates positively with the amount of information he or she receives, up to a certain point. If further information is provided beyond this point, the performance of the individual will rapidly decline [99]. 
One means of managing information overload is through accessing value-added in formation—information that has been collected, processed, filtered, and personalized for each individual user in some way [210]. Lu argues that people rely on social networks for a sense of belonging and interpersonal sources are recognized as more credible and reliable, more applicable, and can add value through intermediate processing and eval uation to reduce information overload. The general public prefers personal contacts for information acquisition [210]. As most of the data is produced and stored in the cloud, users delegate the filtering authority to cloud services. Cloud services are trying to ex tract value and insight from the vast amount of data available, and fine-tune it in order to show what is relevant to their users, often using the users’ interpersonal contacts and social networks. 
For instance, a search engine returns a list of resources depending on the submitted user query. When the same query was submitted by different users, traditional search engines used to return the same results regardless of who submitted the query. In gen eral, each user has different information needs for their query. The user then had to browse through the results in order to find what is relevant for him. In order to decrease this “cognitive overstimulation” on the user side, many cloud services are exploring the use of personalized applications that tailor the information presented to individual users based upon their needs, desires, and recently on who they know in online social net works. Personalized systems address the overstimulation problem by building, man aging, and representing information customized for individual users. Online services achieve this by building a user model that captures the beliefs and knowledge that the system has about the user [122]. In this way the system can predict what will be relevant for the user, filtering out the irrelevant information, increasing relevance and impor tance to an individual user. 
Google uses various “signals” in order to personalize searches including location, previous search keywords and recently contacts in a user’s social network [134]. As Fig ure 1 shows, different users receive different results based on the same keyword search. Facebook on the other hand registers the user’s interactions with other users, the so called “social gestures”. These gestures include like, share, subscribe and comment [330]. When the user interacts with the system by consuming a set of information, the system registers this user interaction history. Later, on the basis of this interaction history, cer tain information is filtered out. For instance content produced by certain friends might be hidden from the user, because the user did not interact with those friends over a pe riod of time. Further, photos and videos receive a higher ranking than regular status posts and some posts receive a higher ranking than others [320]. Personalization algo-
2.3. PERSONALIZATION – A TECHNICAL OVERVIEW 
17 
rithms thus control the incoming information (user does not see everything available), but also determine the outgoing information and who the user can reach (not everything shared by the user will be visible to others). 
Personalization is a kind of information filtering. However, filtering is not a new con cept. During our daily lives we filter information ourselves or delegate the filtering au 
2 
thority to experts, who are called gatekeepers [269]. This is because it would require an unreasonable effort and time for any individual to audit all the available information. The gatekeeper controls whether information passes through the channel and what its final outcome is, which in turn determines the way we define our lives and the world around us, affecting the social reality of every person. Traditional media is used to per form this “gatekeeping” role for news, determining what is newsworthy and important for its audience. However, as information technology and cloud computing are gaining importance, online web services that we use every day are slowly taking over the gate keeping process that used to be performed by the traditional media. 
According to van den Hoven and Rooksby [2008], information is a Rawlsian “primary good”, a good that everybody requires as a condition for well-being. Information objects are means to the acquisition of knowledge and in order to be an autonomous person to plan a rational life, we need information [256]. The more (relevant) data individuals can access in their planning, the more rational their life plan will be. Access to information is, then, a value because it may be instrumental in adding alternatives to one’s choice set, or in ruling out alternatives as unavailable. As a requirement of justice, in high technology information societies, people should be educated in the use of information technologies, and have affordable access to information media sufficient for them to be able to participate in their society’s common life. Bagdikian [2004] similarly argues that media power is political power and the power to control the flow of information is a major factor in the control of society. Giving citizens a choice in ideas and information is as important as giving them choice in politics. 
In 2005, the Pew Internet and American Life Project reported on the rise of search engines, and surveyed users’ knowledge of how they worked. It concluded that “search engines are attaining the status of other institutions—legal, medical, educational, gov ernmental, journalistic—whose performance the public judges by unusually high stan dards, because the public is unusually reliant on them for principled performance” [101]. Personalization and other forms of algorithmic filtering are thus “replacing the tradi tional repositories that individuals and organizations turn to for the information needed to solve problems and make decisions” [234]. The services that employ such algorithms are gateways that act as intermediaries between information sources and information seekers. They play a vital role in how people plan and live their lives. Since access to information is a value, and online filters allow or block access to information, building these algorithms is not only a technical matter, but a political one as well. Before dis cussing how bias can manifest itself in personalization, it is important to first understand how personalization works. 
2.3. PERSONALIZATION – A TECHNICAL OVERVIEW 
Most personalization systems are based on some type of user profile, a data instance of a user model that is applied to adaptive interactive systems. User profiles may include
18 2. BIAS IN ALGORITHMIC FILTERING AND PERSONALIZATION 2   
Figure 2.1: Effects of personalization on Google. First screenshot is with a logged in user from the Netherlands. Second screenshot is from an anonymous user from the Netherlands. Last screenshot is from a logged in user from the US.
2.3. PERSONALIZATION – A TECHNICAL OVERVIEW 
19 
  
2 Figure 2.2: User profile construction for personalization (Adapted from [122]) 
. 
demographic information, (e.g., name, age, country, education level), and may also rep resent the interests or preferences of either a group of users or a single person. In general, the goal of user profiling is to collect information about the subjects in which a user is interested, and the length of time over which they have exhibited this interest, in order to improve the quality of information access and infer the user’s intentions. As shown in Figure 2.2, the user profiling process generally consists of three main phases. First, an information collection process is used to gather raw information about the user. De pending on the information collection process selected, different types of user data can be extracted. The second phase focuses on the construction of a user profile on basis of the user data. Here the collected and stored data are analyzed and processed. In the final phase, the compiled user profile is used in the actual web service, for instance a customized newsfeed in a social networking site, personalized results in a search engine query, or recommended products in an e-commerce site. 
A system can build a user profile in two ways: 
• Explicitly: the user customizes the information source himself. The user can reg ister his interests or demographic information before the personalization starts. The user can also rate topics of interest. 
• Implicitly: the system determines what the user is interested in through various factors, including web usage mining (i.e., previous interaction with the system such as clickthroughs, browsing history, previous queries, time spend reading in formation about a product), IP address, cookies, session id’s, etc. 
Explicit user information collection will allow the user to know that the personaliza tion is taking place and he can tailor it to his needs. However, one problem with explicit feedback is that it places an additional burden on the user. Because of this, or because of privacy concerns, the user may not choose to participate. It is also known that users may not accurately report their own interests or demographic data, or, since the profile remains static whereas the user’s interests may change over time [122]. Implicit user in formation collection, on the other hand, does not require any additional intervention by the user during the process of constructing profiles. It also automatically updates as the user interacts with the system. One drawback of implicit feedback techniques is that they can typically only capture positive feedback. When a user clicks on an item or views a page, it seems reasonable to assume that this indicates some user interest in the item. However, it is not clear that when a user fails to examine some data item it is an indication of disinterest [122].
20 2. BIAS IN ALGORITHMIC FILTERING AND PERSONALIZATION 
Different techniques can be used to make suggestions to users on which information is relevant for them. Recommendation systems try to analyze how a user values certain products or services and then predict what the user will be interested in next. A rec ommendation mechanism typically does not use an explicit query but rather analyses the user context (e.g., what the user has recently purchased or read, and, if available, a 
2 
user profile (e.g., the user likes mystery novels). Then the recommendation mechanism presents to the user one or more descriptions of objects (e.g., books, people, movies) that may be of interest [4, 120]. 
If this recommendation is done solely by analyzing the associations between the user’s past choices and the descriptions of new objects, then it is called “content-based filtering”. Due to increasing user collaboration and user-generated content, personal ization can also be done socially. The so-called social information filtering [292] or col laborative filtering [120] automates the process of “word-of-mouth” recommendations: items are recommended to a user based upon values assigned by other people with simi lar taste. The system determines which users have similar taste via standard formulas for computing statistical correlations [292]. For instance, Facebook uses a collaborative fil tering called Edgerank, which adds a weight to produced user stories (i.e. links, images, comments) and relationships between people [320]. Depending on interaction among people, the site determines whether or not the produced story is displayed in a particu lar user’s newsfeed. This way, a produced story by a user will not be seen by everyone in that user’s contact list. All stories produced by user X can be completely hidden in user Y’s newsfeed, without the knowledge of both users. 
According to Chatman [1987] and Lu [2007], people’s information needs are highly diversified and individualized, making applicable and value-laden information most de sirable, and yet the hardest to obtain. Interpersonal sources can, to a great extent, min imize these difficulties and maximize the utility of information. Even though personal ization technologies such as Grouplens [275] have existed for a while, the rise of social networks and the exponential increase in produced and shared information in online services are changing the impact this technology has. According to Garcia-Molina et al. [2011], information providing mechanisms (e.g. search engines) and personaliza tion systems have developed separately from each other. Personalization systems like recommendation engines were restricted to a single homogenous domain that allowed no keyword search. Search engines on the other hand were geared toward satisfying keyword search with little or no emphasis on personalization or identification of intent. These two systems were separated partly due to a lack of infrastructure. Today, due to a combination of a powerful and cheap back-end infrastructure such as cloud computing and better algorithms, search engines return results extremely fast, and there is now the potential for a further improvement in the relevancy of search results. So, we now see a trend where personalization and information providing mechanisms are blending 
2.4. A MODEL OF FILTERING FOR ONLINE WEB SERVICES Existing work on gatekeeping theory often points out the changing role of the journalist from a gatekeeper to a gatewatcher [48, 294]. With the increasing popularity of the on line media and social networks, every user can share information depending on what he thinks is important. Scholars thus argue that by using online services, the audience can
2.4. A MODEL OF FILTERING FOR ONLINE WEB SERVICES 
21 
exert a greater control over news selection and can focus on issues that they consider more relevant, which in turn empowers audiences and erodes the degree of editorial in fluence over the public’s issue agenda [11]. Some even argue that the gatekeeping role performed by the traditional media becomes irrelevant; gates are disappearing [201]. Information may diffuse through social networks next to mass media channels; there 
2 
fore any audience member can be a gatekeeper for others. Journalists now become a “gatewatcher”, providing a critical analysis of existing topics that are chosen by the com munity [48]. 
Some also claim that the platforms the new “gatewatchers” operate are neutral. Ac cording to Bruns [2011], tools such as Twitter are neutral spaces for collaborative news coverage and curation operated by third parties outside the journalism industry. As a result, the information curated through collaborative action on such social media plat forms should be expected to be drawn from a diverse, multiperspectival range of sources. Also Lasorsa et al. [2012] claim that platforms such as Twitter are neutral communica tion spaces, and offer a unique environment in which journalists are free to commu nicate virtually anything to anyone, beyond many of the natural constraints posed by organizational norms that are existing in traditional media. 
However, as we shall show, the gatekeeping process in online information services is more than a simple transition from editor selection to audience selection or from bi ased human decisions to neutral computerized selections. We argue that human factors play a role not only in the development of algorithms, but in their use as well. We show that factors that caused bias in mass media news selection still play a role in information selection in online web services. Online information intermediaries, similar to the tradi tional media, can control the diffusion of information for millions of people, a fact that gives them extraordinary political and social power. They do not provide equal channels for every user and they are prone to biases. Just as any computer system, they can un fairly discriminate against certain individuals or groups of individuals in favor of others [113]. 
2.4.1. SOURCE SELECTION ALGORITHM 
At the stage of “Collection and Selection” (Figure 2.3), the online service starts to col lect its information from various sources. For instance a search engine will automati cally crawl the web, while the social network site will collect information produced by its users. However, similar to the traditional media where gatekeeping starts with jour nalists [58, 295], algorithmic gatekeeping already starts at source selection. First of all, not all information is digital, thus all non-digital information will be absent from online information intermediaries. Further, not all digitally available information will be avail able to each service, for instance search engines do not index all the data available on the Internet, leading to coverage bias [129, 348]. Google admits that the company does not index every one of the trillion pages on the web, because they are similar to each other or because Google considers some of them not useful to the searcher [131]. Technical reasons can also prevent a search engine to crawl a site. The design of the website might make the source collection and indexing process difficult or the site itself might be ex plicitly blocking the crawling process [31]. Further, if a resource has a bad reputation, for instance if it is suspected as an illegal site, it might be left out of the whole collection
22 2. BIAS IN ALGORITHMIC FILTERING AND PERSONALIZATION 
process. It is also possible that the source does not want to be included in the index due to various reasons. For instance not every page in Facebook or Twitter is indexable by Google [308]. 
2 
2.4.2. INFORMATION SELECTION AND PRIORITIZATION ALGORITHM In traditional media, newspaper editors select some of the messages produced by jour nalist to make news [32]. Algorithms used in web services (such as ranking algorithm in a search engine, or news feed algorithm in a social network) make similar decisions. The design of these algorithms is affected by choices made by designers, i.e., which factors to include in the algorithm, and how to weigh them 1. To serve majority interests, in formation intermediaries often include popularity metric in their ranking algorithm. A search algorithm for instance can give more weight to information coming from popular websites, to support majority interests and values. As a result, seekers will have trouble finding the less popular and smaller sites [245]. 
Because the information filtering is automated, it might be manipulated by activities from third parties. This happens with the so-called “black-hat” Search Engine Optimiza tion (SEO) techniques. This is a method of raising the profile of a Web site with meth ods that Google considers tantamount to cheating [290]. Another factor is own prod uct/service prioritization. The EU recently received a complaint from a shopping search site that claimed it and other similar sites saw their traffic drop after Google began pro moting its own services above conventional search results [8, 93, 94, 363]. Google also integrates content from its social networking platform Google Plus into Google search results, causing protest by the social networking platform Twitter [289]. Studies also showed that Google and Bing search engines both reference their own content in its first results position when no other engine does [93, 364]. Facebook is criticized for favoring the products of its partners [109]. The algorithm can also prioritize certain types of in formation over others. For instance, it is claimed that Facebook treats video and pictures as more important than links and status updates [318]. Similarly, comments on an item are four times more valuable than “likes” [361]. 
In traditional media, regardless of the size of an event such as a public protest, the likelihood that the event will be reported in the media will depend on the current agenda. This is because both print and electronic media regularly focus upon selected issues over a sequence of days, creating the phenomena of “issue attention cycles” [298]. We can ob serve similar behavior in social media. Twitter has a feature called trending topic (TT), in which most popular topics Twitter users are talking about in a particular location are highlighted. However Twitter does not solely check popularity of an item while deter mining TT’s, it favors novelty over popularity. Twitter checks if the user updates on a specific topic is increasing quickly enough. Even if a topic is large volume wise, if the increase rate is small or if it is not novel, it won’t make it to the “trending topics” [326]. This means that it is much easier for a term never seen before to become a Twitter trend and the longer a term stays in the trending topic list, the higher velocity required to keep 
1For instance, Facebook uses an algorithm called Edgerank to determine how a newsfeed of a user is con structed. It is believed that several factors are used to select/prioritize user updates, such as affinity between the receiver and sender, and the date of the published update. However, the exact formula is unknown. See [320]
2.4. A MODEL OF FILTERING FOR ONLINE WEB SERVICES 
23 
it there [209]. This novelty factor caused the hashtag “IcantRespectYouIf” to be a TT in the US, while #OccupyWallStreet not making it to the list. This is because when #Occu pyWallStreet was a TT throughout the world, it had previously trended in the U.S., and now there were no more new people in the U.S. talking about it. 
2 
According to Gillespie [2012], this choice fosters a public more attuned to the “new” than to the discussion of persistent problems, to viral memes more than to slow-building political movements. The exact algorithm that determines the trending topics is un known and this opacity makes the TT, and their criteria, deeply and fundamentally open to interpretation and suspicion [125]. 
TT differs in important ways from those employed in personalization, as it presents itself as a measure of popularity . However, since algorithms such as TT can differ per country, region or city, they might be used to customize content, as an important signal. Popularity can thus be an input to customize items for a group of users. This is still tailored content, but not for an individual, but for a group of individuals. 
Finally, the age of an information source or the age of the information item can also matter. In Google search engine, the number of years a domain name is registered has an impact on search ranking; domain names that exist for a period of time are preferred over newly registered ones [169]. In Facebook, the longer a status update has been out there, the less weight it carries. A news item is prioritized over an old item [320]. This might for instance lead companies to post updates when their audience is most likely to be online and using Facebook. 
2.4.3. HUMAN OPERATOR 
In traditional media, individual factors such as personal judgment can play a role during the selection of news items for a newspaper. An editor’s decisions can be highly sub jective and can be based on the gatekeeper’s own set of experiences, attitudes and ex pectations, leading to a selection bias [119]. Online web services such as search engines frequently claim that such human bias do not exist in their systems. They claim that their core operations are completely automated, but this is false. Humans in online services also make editorial judgments about what data to collect delete or disregard. According to Goldman, online services manually inspect their index and make adjustments [129]. For instance search engines make manual adjustments of a web publisher’s overall rat ing or modify search results presented in response to particular keyword searches [129]. The Dutch newspaper Trouw’s entire domain name and all hosted pages were removed from Google index because of a violation of the company policy [78, 144]. Google itself has admitted that the company manually demotes websites [223]. Similar to blacklisting, search engines can also perform whitelisting. For instance Google recently mentioned that it uses whitelists to manually override its search algorithms [222]. 
Information deletion or withholding is not specific to search engines. Facebook a photo of two men kissing from a user’s Wall due to a violation of the site’s terms of service [374]. There are also claims that Facebook denies and removes advertisements designed for gay audience with no nudity or sexual content, labeling it “inappropriate” [2]. Others claimed that Facebook labeled their posts containing links to a political activism site as spam and prevented the users disseminating this information [22]. Facebook has also removed pages because of offensive content, but later reinstated them [167, 179]. Face-
24 2. BIAS IN ALGORITHMIC FILTERING AND PERSONALIZATION 2   Figure 2.3: A model of filtering for online web services including personalization
2.4. A MODEL OF FILTERING FOR ONLINE WEB SERVICES 
25 
book spokesman blamed the human reviewer in some of the cases, but did not reveal the criteria the company uses on what makes content offensive or in violation with the company’s terms of use. Twitter similarly removes certain ‘trending topics‘ if it considers it as “offensive” [70]. 
2 
Scholars in media studies argued that organizational factors in traditional media play a more important role than individual judgments. In the uncertainty of what tomorrow’s news will be, journalists use so-called routines, patterned, repeated practices and forms, to view and judge in order to define news as predictable events [107]. Similarly, online web services employ operators to delete, withhold or disregard information, to enforce company guidelines. Even though these operators have to obey a set of rules to apply, they have, just like journalists, their own values and can pass personal judgments. This might give the image that the operator is bound to strict rules, and acts merely as an enforcer. However people do not always execute rules in the same way and individual level characteristics are still important [294]. 
Human operators of online services have to evaluate removal requests coming from governments. For instance, recently, A Delhi Court ordered 22 social networking sites (including Facebook, Google, Yahoo and Microsoft) to remove all “anti-religious” or “anti social content and file compliance reports. Google has a list of content removal requests from governments all around the world [132]. Operators also have to deal with requests coming from third parties. For example, Google regularly removes content due to copy right claims coming under the Digital Millennium Copyright Act, Section 512(c). This act gives providers immunity from liability for their users’ copyright infringement, if they re move material when a complaint is received [59]. 
2.4.4. PERSONALIZATION ALGORITHM 
According to Goldman [2005], personalized ranking algorithms reduce the effects of tech nical bias introduced by algorithms in online intermediaries. Goldman argues that per sonalization algorithms increase relevancy and produce a different output per individual user. This in turn diminishes the weight given to popularity-based metrics and reduces the structural biases due to popularity. Personalization might increase relevance, how ever as we show in this subsection, designing only for this value will introduce problems. 
USER INTERACTION HISTORY AND USER PREFERENCES 
As we have argued in Section 3, users could personalize the information they receive by giving their preferences explicitly. In this way they can receive personalized information on the criteria they know. However, if the user’s interests change over the time and if the user does not update their filter, they might miss some information that might be of in terest to her. Lavie et al. [2009] found that people might be interested in things that they did not know they were interested in, due to the formulation of the topic. Some users have asserted that they were not interested in politics, but later it was shown that their perception of “politics” was limited to local politics. They later have shown interest in in ternational politics [198]. Lavie et al. argue that, overall, users cannot accurately assess their interests in news topics. Similarly Tewksbury [2003] reports that user’s declared and actual interests may differ. 
In his book Republic.com, Sunstein [2002] developed his concern that explicit per-
26 2. BIAS IN ALGORITHMIC FILTERING AND PERSONALIZATION 
sonalization will assist us to avoid facts and opinions with which we disagree, leading people to join online groups that conform with their existing beliefs. Since democracy is most effective when citizens have accurate beliefs and to form such beliefs, individu als must encounter information that will sometimes contradict their preexisting views. Sunstein argues that explicit personalization will undermine deliberative democracy by 
2 
limiting contradictory information. 
Implicit personalization using user interaction history has its own concerns. Pariser [2011] argues that online services can cause citizens to be ill-informed about current events and may have increasingly idiosyncratic perceptions about the importance of current events and political issues. This might occur because online services are try ing to improve accuracy at the expense of serendipity, leading to what Pariser calls “filter bubble”. Even if users wanted to diversify their network explicitly, information inter mediaries silently filter out what they assume the user does not want to see, hiding in formation posted by opposite end of political spectrum. For Sunstein, explicit excessive personalization leads to never seeing the other side of an argument and thus fostering an ill-informed political discourse. For Pariser, excessive implicit personalization leads to an unhealthy distaste for the unfamiliar. The problem is thus an automatic cyberbalka nization, not an “opt-in” one. It happens behind the scenes and we do not know what we are not seeing. We may miss the views and voices that challenge our own thinking 
Pariser argues that online personalization algorithms are designed to amplify con firmation bias, Consuming information that conforms to our beliefs is easy and plea surable; consuming information that challenges us to think differently or question our assumptions is difficult. Pariser notes that we all have internal battles between our as pirational selves (who want greater diversity) and our current selves (who often want something easy to consume). Pariser argues that the filter bubbles edit out our aspira tional selves when we need a mix of both. Pariser believes that the algorithmic gate keepers need to show us things that are not only easy to consume but also things that are challenging, important and uncomfortable and present competing points of view. Pariser states that filter bubbles disconnect us from our “ideal selves”, that version of ourselves that we want to be in the long-run, but that we struggle to act on quickly when making impulse decisions. 
LOCATION 
As we have shown in Section 3, content can also be personalized based on location. Large Web-search engines have been “personalizing” search to some extent for years. Users in the U.K. will get different results searching for certain terms, especially com mercial ones, than users in the U.S. Results can change between different cities as well [120]. The idea is that the user will be more interested in local content. However, this will depend on context of information. For instance, if I am looking for a restaurant, I would want my search engine to personalize results based on location, the system should show me pizzerias in Rotterdam, but not in New York. However, if I am looking for some tech nical information in a forum to solve a PC problem, then I do not necessarily care about the location (if I can speak multiple languages). Currently, most personalization systems filter information based on location without taking the context into the account. This might always favor local content, even if the quality or the relevance of the local content is inferior to a non-local content.
2.4. A MODEL OF FILTERING FOR ONLINE WEB SERVICES 
27 
AUDIENCES 
While traditional news media outlets want to satisfy their readers and viewers, it is much more difficult for them to modify their selection criteria in real time, than it is for on line gatekeepers. Online gatekeepers have immediate feedback about what queries are 2 
issued, what content is selected and what sites are accessed. For instance online ser vices can observe user behavior through entered queries or clicked links to modify its algorithms accordingly. However, online services can also capture user’s intent by us ing social gestures. Examples of these social gestures include the “like” and “subscribe” buttons in Facebook and the “+1” button in Google search. By clicking on these buttons users express their interests and see what item is popular. Google currently does not use these (anonymous) votes to personalize search results, but such approaches are well known in computer science literature. Search behavior of communities of like-minded users can be harnessed and shared to adapt the results of a conventional search en gine according to the needs and preferences of a particular community [300]. Because similarities will exist among community members’ search patterns and web search is a repetitive and regular activity, a collaborative search engine can be devised. This human PageRank or “social-graph”, using +1 results to give context to the popularity of a page, can be a supplement (or alternative) to the link graph Google is currently using. 
Some claim that the community is wiser than the individual. However, community driven filtering has its own problems. For example, in social news aggregator Reddit, where anonymous users submit links to items, comment on them, vote on the submitted items and comments, the community determines what is newsworthy, for every topic. Users can personalize their news feed by explicitly subscribing to certain subtopics, but the popularity metric is used in every subtopic. In Reddit, the timing of the story sub mission is important. If a good news item is submitted outside of Internet prime-times, it will not receive enough votes to make it to the front page. The result is that most sub missions that originate in the US end up being dominated by US comments, since new comments posted several hours after the first will go straight to the middle of the pile, which most viewers will never get to. Submission time has a big impact on the ranking and the algorithm will rank newer stories higher than older. In Reddit, first votes also score higher than the rest. The first 10 upvotes count as high as the next 100, e.g. a story that has 10 upvotes and a story that has 50 upvotes will have a similar ranking. Contro versial stories that get similar amounts of upvotes and downvotes will get a low ranking compared to stories that mainly get upvotes [281]. Further, the user will receive positive or negative points on the story he submitted. The individual might remove the story due to decreasing points in his reputation. 
It is also known that in such vote-based social news sites, the amount of contacts or followers one has can also determine whether his story will make it to the front page. Having a large number of contacts will make it easier to reach the front page (more friends, more votes). Also, some social news aggregators divide the stories into topics. If a topic has a small number of subscribers, the chance that it will make it to front page is small [181]. Even the items that do not make it to the front page will bring traffic to the submitted site. Therefore social news aggregators like Reddit are being used and manip ulated by online marketing professionals, in order to draw more traffic to their products or services. Similarly, Facebook’s like button can also be gamed. Digital marketing com-
28 2. BIAS IN ALGORITHMIC FILTERING AND PERSONALIZATION 
panies can create fake users and buy “friends” and “likes” [327]. These companies use software to automate clicking the “Like” button for a certain page. Such software can by pass Facebook’s security system. If popularity is devised by only the number of likes and used as an input for users in a certain region, it can also cause bias in personalization. 
2 
INTERPERSONAL NETWORKS 
According to Chen and Hernon [1982], the general population tends to obtain informa tion through interpersonal networks, rather than formal means [56]. Durrance [1984] found that more than 64% of her research participants used interpersonal sources [92]. Sturges maintains that there is a “fundamental preference for information mediated by human interaction” and that “there is evidence of this from all parts of the world and from most important aspects of human life” [307]. Katz and Lazarsfeld [2005] argue that we live in communities and we are inherently tied to different social connections. We interact in formal or informal social groupings, in so-called “primary groups” such as families, friends, work teams, clubs or organizations. These primary groups delineate major life boundaries for each one of us in society, our routine activities mainly occur in these primary groups. 
Since our lives are mainly contained in primary groups, our attitudes and opinions tend to derive from them as well as our sources of information. Primary groups pro vide us with “social reality” to validate our actions. As we encounter unknown situations and difficult decisions, we turn to and consult our social contacts, including both strong (e.g., family and friends) and weak ties (e.g., colleagues, acquaintances) to help us form opinions and find solutions [136]. Lu [2007] argues that, through interactions concern ing a particular issue, a primary group tends to develop a common view and collective approach, hence, provides a social reality that helps and validates decision making by its members. Because members of a primary group share the community language and background information, their communication is made effortless. Information so trans mitted becomes easily accessible and digestible [210]. 
Because of these reasons, instead of relying on user’s explicit preferences, or using an anonymous popularity metric, personalization services started to use interpersonal relationships to filter information. For instance Facebook launched a program called “instant personalization” with an exclusive set of partners, including the restaurant ag gregator site Yelp, Microsoft online document management site docs.com, customizable Internet radio sites Pandora and Spotify. These partners have been given access to public information on Facebook (e.g., names, friend lists, and interests and other information users have shared on their Facebook profiles) to personalize a user’s experience on the partner’s site. As an example, online music service Spotify requires a Facebook account, and using the friends list in Facebook, it shows the user what her friends have listened to. The idea here is, since these contacts are part of our primary group, we can trust their judgment on which information is newsworthy. If our primary groups are available in every web service we use, then our experience using that web service can be customized. 
Similarly Google introduced social search in 2009, personalizing search results based on people you know in Facebook and Twitter, rather than your personal behavior. As a latest move, in 2012, Google introduced a feature called “Search plus your world”. This feature personalizes the results using user connections in Google Plus, Google’s social
2.5. DISCUSSION 
29 
networking platform. This means you might see a picture of a friend’s car when you search for a new automobile, or a restaurant recommended by a friend when you search for a place to eat. Even if you aren’t a Google+ user, Google search results will show content posted publicly on the social network that it judges to be relevant —profile pages and pages dedicated to particular topics [182]. 
2 
ADVERTISERS 
Traditional mass media is primarily supported by commercial sponsorship. This can cause the newspapers to delete, change or prioritize news items due to advertising pres sure [301]. Same pressure applies to online services; the majority of online service rev enues come from advertising [247, 287, 331]. Personalization is a very attractive tool for advertisers, as user data collected for information filtering can be used for behav ioral targeting. This sort of online targeting provides more relevant online advertising to potential upcoming purchases. Using the built up user profile in online services, adver tising networks can closely match advertising to potential customers. According to Guha et al. [2010], Facebook uses various profile elements to display targeted advertisement including age, gender, marital status, and education. A Facebook advertiser can target users who live within 50 miles of San Francisco, are male, between 24-30 years old, sin gle, interested in women, like skiing, have graduated from Harvard and work at Apple [184]. Google allows advertisers to target ads based not just on keywords and demo graphics, but on user interests as well [252]. Companies have recognized that providing advertisements along with their recommendations (suitably distinguished from the rec ommendation results) can be extremely profitable. For instance, the auction site Ebay provides a “deal of the day” for all visitors to the site, in addition to “buy it now”, spe cial items directly sold from a provider for a fixed price—both of these are essentially advertisements [120]. 
PRESENTATION ALGORITHM 
Once information is chosen through the information selection algorithm and personal ized for the user, it does not mean that it will be seen and consumed. The placement of the information might determine if it makes it out of the filter. Joachims and Radlinski [2007] show that the way a search engine presents results to the user has a strong influ ence on how users act. In their study, for all results below the third rank, users did not even look at the result for more than half of the queries. Bar-Ilan et al. [2009] report sim ilar findings. Yue et al. [371] report that the attractiveness of information can also cause presentation bias if the title and abstract of a resource is bolded, it generates more clicks. They also show that people tend to click on the top and bottom results. These findings show that what the user will consume can be affected by the algorithm, even after source selection and personalization. 
2.5. DISCUSSION 
2.5.1. IMPLICATIONS FOR AN ETHICAL ANALYSIS 
Personalization is the latest step in this algorithmic filtering process. As we have argued, even though personalization algorithms have existed since the 1990’s, information pro viding services such as search engines did not contain such algorithms until recently.
30 2. BIAS IN ALGORITHMIC FILTERING AND PERSONALIZATION 
This is mainly due to the recent availability of cheap and powerful backend infrastruc ture and the increasing popularity of social networking sites. Today information seeking services can use interpersonal contacts of users in order to tailor information and to in crease relevancy. this not only introduces bias as our model shows, but it also has serious implications for other human values, including user autonomy, transparency, objectiv 
2 
ity, serendipity, privacy and trust. These values introduce ethical questions. Do private companies that are offering information services have a social responsibility, and should they be regulated? Should they aim to promote values that the traditional media was ad hering to, such as transparency, accountability and answerability? How can a value such as transparency be promoted in an algorithm? How should we balance between auton omy and serendipity and between explicit and implicit personalization? How should we define serendipity? Should relevancy be defined as what is popular in a given location or by what our primary groups find interesting? Can algorithms truly replace human filterers? 
A relevant value to bias is information diversity. For instance if a search engine is exercising bias toward an advertiser, it will be limiting the diversity and democracy in herent to the information [135]. Information diversity is a rich and complex value that can be conceptualized in many different ways, and its interpretation differs significantly per discipline. In media studies, it might be translated as “minority voices having equal access in the media ” or “the degree which the media relates to the society in such a way to reflect the distribution of opinion as it appears in the population” [335]. In Com puter Science literature, it can be defined as “variety in the products offered by the sys tem”, “helping user find items he cannot easily find himself” [373] or “identifying a list of items that are dissimilar with each other, but nonetheless relevant to the user’s interests” [370]. While media studies are analysing this ethical value in detail, almost all scholars of search engine diversity seem to be limiting their understanding of “bias” and “diver sity” to popularity bias [135]. As our model shows, popularity is only one of the many factors that cause bias. We need a normative conceptualization of the value information diversity that borrows notions from media studies, such as media ownership, content di versity, viewpoint diversity, reflection and open-access [335]. Only then can we translate this complex value into design requirements of information intermediaries and move towards a solution. 
We believe that normative arguments based on our model will be stronger, more con crete and constructive. As an example, take the value user autonomy. Autonomy is cen trally concerned with self-determination, making one’s own decisions, even if those de cisions are sometimes wrong [114]. Autonomy is thus the individual’s ability to govern herself, be one’s own person, to be directed by considerations, desires, conditions, and characteristics that are not simply imposed externally upon one, but are part of what can somehow be considered one’s authentic self [61]. It is this aspect of decision-making that allows us to be responsible for the consequences of our actions. While designing technology, one can thus assume that designers should maximize user autonomy by fol lowing the simple dictum that more control leads to more user autonomy. After all, if au tonomous individuals need to have freedom to choose ends and means, then it could be said that wherever possible and at all levels, designers should provide users the greatest possible control over computing power. Considering this notion of autonomy, one could
2.5. DISCUSSION 
31 
argue that personalization algorithms should always be fully customized and should be based on explicit personalization. However, as the model shows, explicit personalization based on user preferences is also prone to bias. People might be interested in things that they did not know they were interested in, due to the formulation of the topic. Further, users might not accurately assess their interests in certain information items. As we have 
2 
mentioned, user’s declared and actual interests may differ. 
This seems to suggest that autonomy in this context should not be understood as “full user control”. User autonomy seems to have less to do with simply the degree of control and more to do with what aspects of the algorithm are controllable, and the user’s con ception and knowledge of the algorithm. As Friedman and Nissenbaum note, achieving higher order desires and goals will enhance autonomy, whereas excessive control may actually interfere with user autonomy by obstructing a user’s ability to achieve desired goals [114]. This means that, implicit personalization must be combined with explicit personalization to decrease excessive control. For instance a personalized search en gine might be implemented in such a way that, the system enters a dialogue with the user, explicitly stating that a certain query is personalized, explaining why and due to which reasons it is personalized. The system can thus make assumptions to predict what the user might like, but it should refine itself by asking simple questions to the user to confirm if those assumptions were correct. While the user might not control the full al gorithm, the system might receive feedbacks and show the user under which conditions it is making certain recommendations. 
As we have argued, information should be accepted as a primary good, a vital good for people to plan their lives rationally and to participate adequately in the common life of their societies [344]. Thus, having access to information affects the value of liberty perceived by an individual. We therefore argue that personalizing algorithms affect the moral value of information as they facilitate an individual’s access to information. Con trary to earlier stages of the Internet-era, when the problem of information access boiled down to having access to hardware, today the problem of access to information concerns the ability to intentionally find the right information, or the likeliness of unintentionally stumbling upon the relevant information. 
Some argue that users should sabotage the personalization system by deliberately clicking on links that make it hard for the personalization engines, erasing cookies, un locking everyone on a social network, posting something and then ask the Facebook friends to click the “Like” button and comment, or simply switch to a service that does not use personalization [96, 256]. However, these tactics are tedious, not always possi ble to perform and their effect depends on the implementation of the current system. Further, personalization might actually have a positive effect on the ecology of the cy berspace: the incentives to game the system and invest in practices like “search engine optimization“ can become weaker [129, 231]. We should come with design suggestions to minimize the bad effects and improve the good effects of this technology instead of trying to get rid of it all together. 
The question is then not whether to have personalization or not, but how to design morally good personalization technology. ‘Having too much information with no real way of separating the wheat from the chaff’ is what Benkler [2006] calls the Babel objec tion: ‘individuals must have access to some mechanism that sifts through the universe of
32 2. BIAS IN ALGORITHMIC FILTERING AND PERSONALIZATION 
information, knowledge, and cultural moves in order to whittle them down into manage able and usable scope’. The question then arises whether the service providers currently active on the Internet are able to fulfill the ‘human need for filtration’. Although the fulfillment does not hinge on proprietary services alone as there are cooperative peer production alternatives that operate as filters as well, the filtering market is dominated 
2 
by commercial services such as Google and Facebook [163]. Having an option to turn it on or off is not really a choice for the users, as they will be too dependent on it in the existence of information overload. 
2.5.2. IMPLICATIONS FOR DESIGN 
In order to anticipate different contexts of use in personalization, a value based study such as Value Sensitive Design [108, 116] seems to be the right direction. Value Sensi tive Design (VSD) consists of an empirical investigation accompanied by a philosophical analysis and a technical study. Friedman and Nissenbaum [1996] argue that designers should not only envision a system’s intended situation of use, but to account for increas ingly diverse social contexts of use. Designers should then reasonably anticipate prob able contexts of use and design for these. If it is not possible to design for extended contexts of use, designers should attempt to articulate constraints on the appropriate contexts of a system’s use. Bias can manifest itself when the system is used by a popu lation with different values than those assumed in the design. This is especially true for the design of most online information intermediaries, where users from the whole world will be served instead of only local ones. 
Another issue that is relevant to the design of personalization algorithms and other filtering mechanisms is exposure diversity. Even if an information intermediary provides a balanced information diet, this does not guarantee that the user will actually consume this information [157, 238, 243]. Content diversity is not equal to exposure diversity. We need to devise methods to increase the consumption of challenging content by users. Munson and Resnick [2010] distinguished two types of users: challenge averse (those who ignore diverse content) and diversity seeking. They tried to show more diverse con tent to those who were challenge averse, for instance by highlighting agreeable items or showing agreeable items first. However, this did not increase users’ consumption habits, they still ignored challenging items. This requires us to research further how challeng ing items can be made attractive to users so that they actually consume the incoming information. 
2.5.3. IMPLICATIONS FOR THE DESIGN OF SOCIAL FILTERING 
Media scholars often argue our interpersonal contacts have become our gatekeepers [294]. However, if this approach becomes ubiquitous in design, it can lead to prob lems. First, this obviously raises concerns for privacy. An item a user has consumed can be shared with others without their notice. The Electronic Privacy Information Cen ter, American Civil Liberties Union and American Library Association claim the changes have made sharing information on Facebook a passive rather than active activity. In this way, users might reveal more than they intend [241]. Even if sharing process was more active, it can still cause issues. For instance, an item a user has shared in a social network in certain context and has forgotten can reappear in a Google search result in a different
2.5. DISCUSSION 
33 
context. Further, an implicit user profile built for personalization leads to epistemologi cal problems. Does the knowledge about the user (gathered by user’s interaction with the system) represent the reality? Does the user interact with its primary group the same way he interacts in the offline world? How much does a user have a say in this built profile and to what degree can he control the dissemination of this representation of himself? 
2 
Second, not everyone in our online social networks will be part of our primary group; not every online “friend” is our real friend and we might share different things with our online friends. We sometimes add people to our network because of courtesy, as it oth erwise might cause relationship problems in the offline world (“Why did you not an swer my friend request?”). To remedy this, we can arrange the level of our relationship with others in a social network; we can divide them into lists or groups. We can then choose what we want to share with which group. However, our contact list in a social network can be connected with a different service, for personalization. When we use our social network in another service, lists we have created can suddenly disappear. For instance, Spotify uses Facebook contact list to provide recommendations per individual user. However, it ignores all the lists that have been created and shows what all friends have listened to regardless of the relationship between the user and the friend. The cat egorization the user has set in the Facebook platform in order to define and control his relationships are gone when the Facebook data is used elsewhere. Next to increasing in formation overload, this can also cause privacy issues. Even if I choose to share things with some people in Facebook context, everything I listen to in Spotify will be shown to all my Facebook users. This context loss will be more common as more services integrate with each other. 
Third, not everyone has competence on every subject. Scholars in various disciplines have found that there are strategic points for the transmission of information in every group [5, 55, 210]. Even though it is possible that people can interact randomly with anyone who has available information, information transmission is never a simple ag gregation [177, 296]. Some individuals, who are more information-savvy, will automati cally occupy strategic positions to facilitate access to information to others. Depending on the subject matter, not everyone in a group is equally important or qualified in pro viding information. Those who have more knowledge will act as gatekeepers. I might trust John’s competence in football, and use him as my gatekeeper in this subject, but not in the area of international politics. However, in most online services, we get to see everything published by a user, or nothing at all. We need mechanisms to assess the competency of the information sharer and determine the needed gatekeeper for a given context. 
Fourth, online services are trying to capture user’s intent by using social gestures. Ex amples of these social gestures include the “like” and “subscribe” buttons in Facebook and the “+1” button in Google search. By clicking on these buttons users express their in terest and communicate to their peers. However, this sort of expression seems somehow limiting [256]. The reason of the expression and the emotion behind the expression is not captured by the button. There is a difference between liking a film, liking a director, liking a genre or liking films of a certain period. I might like a film for various reasons: to recommend to friends, to express my identity, to receive further film recommenda tions or to add it into my collection for later use. Such buttons are simplifying complex
34 2. BIAS IN ALGORITHMIC FILTERING AND PERSONALIZATION 
human actions and emotions into a single dimension. As Friedman and Nissenbaum [1996] have argued, attempting to formalize human constructs such as discourse, judg ments, or intuitions and trying to quantify the qualitative, discretizing the continuous will lead to biases. 
2 
Fifth, online services assume that users want to have an online experience where consuming any sort of information is done socially and collaboratively. This is why Google is making social search the default type of search and Facebook persuades users to share more information or leave a trace of a completed activity, by its “frictionless sharing”. These approaches aim to make sharing an effortless activity, in which every thing is shared and hopefully some things will be found interesting by the users. How ever by promoting ease, they are undermining not only privacy, but also autonomy. In a frictionless sharing environment, user now cannot actively reflect on things he con sumes and choose on what to share. 
Finally, if we know the information we consume is being shared and read by our pri mary groups, we might change our behavior on what to share, and even choose what to consume if this is shared automatically. According to Sunstein [2008], group mem bers may fail to disclose what they know out of respect for the information publicly an nounced by others. That is, even if we have big doubts about claims made by the majority of a group, we might think they are not errors at all; not so many people can be wrong. Individuals can also silence themselves to avoid the disapproval of peers and supervi sors. As a result of these two forces, information cascades might occur; individual errors might amplify instead of being corrected, leading to widespread mistakes. Information held by all or most will be prioritized over held by a few or one. 
2.5.4. IMPLICATIONS FOR SOCIAL NETWORK ANALYSIS 
While bias might manifest itself in the social platform, users themselves might be biased in information sharing. Therefore we need to determine whether bias occurs naturally in social networks, as personalization algorithms use more and more social data. Do users tend to follow like-minded users? Do they do this intentionally? Do they only share things that they agree with? Do they receive diverse information directly or indirectly? Do they only want to follow popular items coming from major news sources as the cur rent services, or does the minority receive a chance to contribute to the debate? Is the sharing behaviour of the user changing with what he is receiving? Does culture have an affect in diverse information seeking behaviour? To answer such questions, we need to perform more empirical studies. 
Facebook performed one of the few studies that actually studies bias in social net works [26]. The empirical study suggests that online social networks may actually in crease the spread of novel information and diverse viewpoints. According to Bakshy [2012], even though people are more likely to consume and share information that comes from close contacts that they interact with frequently (like discussing a photo from last night’s party), the vast majority of information comes from contacts that they interact with infrequently. These so-called “weak-ties” [136] are also more likely to share novel information. 
Even though this is one of the first empirical studies that aims to measure informa tion diffusion, there are some concerns with it: First of all, the study is not repeatable
2.6. CONCLUSION 
35 
and the results are not reproducible. Facebook scientists simply manipulated newsfeed of 253 million users, which only Facebook can perform. Second, our weak ties give us access to new stories that we wouldn’t otherwise have seen, but these stories might not be different ideologically from our own general worldview. They might be new informa tion, but not particularly diverse. The research does not indicate whether we encounter 
2 
and engage with news that opposes our own beliefs through links sent by “weak links“. It could very well be that we comment on and re-share links to cat videos sent by our previous neighbour, or read a cooking recipe posted by our vegetarian friend, ignore anything political or challenging/contradictory to our world view. The study measures the amount of different information one gets, not different world-views. Third, the users might refrain from novel information if they consider it to be offensive or distasteful to their (strong or weak) ties. Fourth, even if users are shown novel information, this does not mean they will be exposed to it. They might simply choose to ignore challenging items. Fifth, the information intermediary might filter out the novel content provided by our weak ties. If, for instance, Facebook decides which updates you see on your wall based on the frequency of an interaction, weak ties might as well disappear, as the user will not interact very often with a weak tie. At the moment the only way to prevent this is to manually click on each and every user and choose “show me all updates from this user”. Otherwise Facebook will make a decision on what is important based on some unknown criteria. 
2.6. CONCLUSION 
Gatekeeping theory acknowledges the increasing popularity of social networking, on line information seeking and information sharing services. It is often claimed that since users can select and share information online, they can be gatekeepers for each other. This then diminishes the power of media professionals. However, in this paper we have shown that even though the traditional gatekeepers might become less important, users are not becoming the sole gatekeepers. The gates are certainly not disappearing. Plat forms on which users operate have an influence; they are one of the new gatekeepers. Online gatekeeping services are not just algorithms running on machines; they are a mix of human editors and machine code designed by humans. People affect the design of the algorithms, but they also can also manually influence the filtering process after the algorithm has been designed. Therefore, switching from human editing to algorithmic gatekeeping does not remove all human biases. Technical biases such as third party ma nipulation or popularity will exist due to the computerized form of gatekeeping. Also, individual factors such as personal judgments, organizational factors such as company policies, external factors such as government or advertiser requests will still be present due to the role of humans in providing these services. 
In this paper, we introduced a model of algorithmic gatekeeping based on traditional gatekeeping model and focused on particular filtering processes including personaliza tion. We show that factors that caused bias in mass media news selection still play a role in information selection in online web services. We have shown that search results in Google can differ, but an extensive empirical research is needed to determine the ex tent of so-called “echo chambers” in social networks. What percentage of information do users miss or feel like they are missing if they turn on a personal filter or an inter-personal
36 2. BIAS IN ALGORITHMIC FILTERING AND PERSONALIZATION 
filter? Is there enough variety in their choice of friends? Are users aware of these algo rithms? Do they modify their filter periodically or switch to other forms of information sources? Are there routines that are used in the design of personalization algorithms, just like routines used in traditional gatekeeping? How does the introduction of implicit and explicit filtering algorithms affect user trust in systems and user autonomy? More 
2 
research is needed in order to answer these questions.
3 
VIEWPOINT DIVERSITY IN ONLINE SOCIAL NETWORKS - AN 
EMPIRICAL ANALYSIS 
One of the things that amazes me about Twitter is the way it utterly eradicates artificial barriers to communication. Things like status, geopolitics and so on keep people from talking to one another. Those go away in Twitter. 
Dick Costolo, CEO of Twitter 
3.1. INTRODUCTION 
It is well known that traditional media have a bias in selecting what to report and in choosing a perspective on a particular topic. Individual factors such as personal judg ment can play a role during the selection of news for a newspaper. Selection bias, orga nizational factors, advertiser and government influences can all affect which items will become news [45]. About 37% of Americans see a great deal of political bias in news cov erage and 68% percent prefer to get political news from sources that have no particular point of view [265]. Similarly, in a survey performed be fore general elections in the UK, 96% of the population said they believe they have seen clear bias within the UK media [351]. Evidence of bias ranges from the topic choice of the New York Times to the choice of think-tanks that the media refer to [79]. 
Many democracy theorists claim that modern deliberative democracy requires cit izens to have socially validated and justifiable preferences. Institutional designs must show particular attention to procedures of preference formation and learning within politics and civil society. Citizens must be exposed to opposed preferences and view points and be able to defend their views [90, 158, 248]. Exposure to biased news infor mation can foster intolerance to opposing viewpoints, lead to ideological segregation 
This chapter has been published in Computers in Human Behavior [98] 
37
38 3. VIEWPOINT DIVERSITY IN ONLINE SOCIAL NETWORKS - AN EMPIRICAL ANALYSIS 
and antagonisms in major political and social issues [14, 127, 280]. Being aware and overcoming bias in news reporting is essential for a fair society, as media has the power to shape voting behavior and a democratic society [280]. 
Social information streams, i.e., status updates from social networking sites, have emerged as a popular means of information awareness. Political discussions on these platforms are becoming an increasingly relevant source of political information, often used as a source of quotes for media outlets [171]. Traditional media are declining in their gatekeeping role to determine the agenda and select which issues and viewpoints 3 
should reach their audiences [49]. Internet users have moved from scanning traditional mediums such as newspapers and television to using the Internet, in particular social networking sites [14]. Social networking sites are thus acting as gatekeepers [45]. A small number of users, who are critically positioned in the structure of Twitter, can determine political communication with their own political perspectives [171]. These new gate keepers exert strong and selective influence on the information passed within Twitter. 
It is often argued that the Internet, by promoting equal access to diverging prefer ences and opinions in society, actually increases information diversity. Many scholars characterize the online media landscape as the "age of plenty", with an almost infinite choice and unparalleled pluralization of voices that have access to the public sphere [174]. Some argue that social media will disrupt the traditional elite control of media and amplify the political voice of non-elites and minorities [53]. The expansion of choice and participatory nature of the Internet not only ends the “scarcity” and “concentration” problems faced by the traditional media, but it also diminishes the power of gatekeepers. 
Others claim that tools such as Twitter are neutral spaces for collaborative news cov erage and curation operated by third parties outside the journalism industry. As a result, the information curated through collaborative action on such social media platforms should be expected to be drawn from a diverse, multi-perspectival range of sources [49]. Some further claim that platforms such as Twitter are neutral communication spaces, and offer a unique environment in which journalists are free to communicate virtually anything to anyone, beyond many of the natural constraints posed by organizational norms that are existing in traditional media [196]. Some argue that digital tools such as social media will inevitably lead to the pluralization of the public sphere and non mainstream political actors can influence the political agenda thanks to the “multiaxity" power of those platforms [358]. 
On the other hand, there are skeptical voices that argue that the Internet has not fundamentally changed the concentrated structure typical of mass media, but reflects the previously recognized inequalities [174]. It is also argued that it has brought about new forms of exclusion and hierarchy [316]. While it has increased some sort of political participation, it has empowered a small set of elites and they still strongly shape how political material is presented and accessed [162]. Others have pointed out the danger of “cyberbalkanization" caused by the Internet [256, 312]. They argue that the filters we choose on the internet, or the filters that are imposed upon us will weaken the demo cratic process. This is because it will allow citizens to join into groups that share their own views and values, and cut themselves off from any information that might challenge their beliefs. 
Group deliberation among like-minded people can create polarization; individuals
3.2. EMPIRICAL STUDIES OF INFORMATION DIVERSITY IN SOCIAL MEDIA 39 
may lead each other in the direction of error and falsehood, simply because of the lim ited argument pool and the operation of social influences. Increased polarization makes it more difficult for society to find common ground on important issues [312]. Research shows that ‘confirmation bias’ occurs when like-minded individuals form a group in or der to make a decision [288]. When participants receive new information in a decision case after they have reached a preliminary conclusion, a clear preference was demon strated for information supporting the preliminary group decision. Finally, the ability of online intermediaries such as recommender systems and social networks to customize their items to the taste of individuals, together with users’ preference to reading opin 
3 
ions which reinforce their own viewpoints, raises the phenomenon referred to as “filter bubble" [256]. 
In short, there is little consensus on whether the technological and socio-cultural changes in online media have actually increased diversity and plurality [174]. The inter esting question is whether cyberbalkanization indeed occurs on online social networks. There are empirical studies that have observed a high level of information diversity in Twitter and Facebook, mainly due to retweets and weak-links [15, 26, 310]. While being very valuable contributions to the literature, these studies often focus on American users and they define information diversity either as “novelty", or “source diversity". However, as we will show later, novel information does not necessarily contribute to information diversity and highly competitive media markets with many sources may still result in excessive sameness of media contents. As we will argue, marginalized members of seg regated groups, structurally underprivileged actors and minorities must receive special attention and just measuring number of available sources will not guarantee viewpoint diversity. 
In this paper, in order to understand the impact of political culture, we analyze data from hundreds of political information sources and their followers in Twitter for two dif ferent countries. Naturally, we do not expect that the concept information diversity can be reduced to a single quantity or metric. Therefore, we first introduce different defi nitions of information diversity using the theory from communication studies and po litical philosophy. We provide a set of metrics that are based on this theory. Finally we present the result of an empirical study we performed using these metrics. Our main contributions are the following: 
• We perform a conceptual analysis of the value “information diversity". 
• We demonstrate multiple metrics that capture different biases in a large sample of Twitter users from the Netherlands and Turkey. 
• We show that, if information diversity is defined as “minority access", we do ob serve “bubbles" in users’ newsfeeds 
3.2. EMPIRICAL STUDIES OF INFORMATION DIVERSITY IN SO CIAL MEDIA 
An empirical study performed by Facebook suggests that online social networks may increase the spread of novel information and of diverse viewpoints. According to Bak shy (2012), even though people are more likely to consume and share information that
40 3. VIEWPOINT DIVERSITY IN ONLINE SOCIAL NETWORKS - AN EMPIRICAL ANALYSIS 
comes from close contacts that they interact with frequently, the vast majority of infor mation comes from contacts that they interact with infrequently. These so-called "weak ties" [136] are also more likely to share novel information. However, there are some con cerns with this study. First, Facebook does not provide open access to everyone, thus we may not repeat or reproduce the results using Facebook data. Second, our weak ties give us access to new stories that we would not otherwise have seen, but these stories might not be different ideologically from our own general worldview. They might be novel in formation, but not particularly diverse. The concepts serendipity, diversity and novelty are different from each other [310]. The Facebook research does not indicate whether 
3 
we encounter and engage with news that opposes our own beliefs through links sent by "weak links". 
Twitter, with its API, provides an excellent environment for information diversity re search. An et al [2012] observe extreme polarization among media sources in Twitter. In another study, they found that, when direct subscription is considered alone, most Twitter users receive only biased political views they agree with [15]. However, they note that the news media landscape changes dramatically under the influence of retweets, broadening the opportunity for users to receive updates from politically diverse media outlets. Sun et al [2013] performed an empirical study using statistical models to identify serendipity in Twitter and Weibo. Using likelihood ratio test and by measuring unexpect edness and relevance, they observe that serendipity has a strong presence in information diffusion in microblogging communities. Saez-Trumper et al. [2013] found that political bias is evident in social media, in terms of the distribution of tweets that different sto ries receive. Further, statement bias is evident in social media; a more opinionated and negative language is used than the one used in traditional media. Twitter users are more interested in what is happening around them and what is happening to those around them. While communities talk about a broad range of news, Twitter users dedicate most of their time to a few of them [280]. Wei et al. [2013] found out that individual journal ist have the strongest influence on Twitter for UK users. Further, they observed that all influential British Twitter users (mainstream media, journalists and celebrities) display some kind of bias towards a particular political party in their tweets. 
3.3. THEORY 
In this section, we first give a short overview “information diversity" and explain why it is a vital value for a democratic society. Later, we show different dimensions of this value and show how it can be defined. 
3.3.1. INFORMATION DIVERSITY 
A cyberbalkanized Internet or “filter bubble" is not acceptable in a deliberative, liberal democracy. Whereas aggregative versions of democracy hold that legitimacy lies in the fair counting of votes for and against a decision, deliberative democrats hold that a deci sion is only legitimate if it is determined by a fair, informed discussion [106]. Because no set of values or preferences can claim to be correct by themselves, they must be justified and tested through social encounters which take the point of view of others into account [158]. In addition to the normative value of discussion, information-sharing is required
3.3. THEORY 
41 
for many of the practical benefits that proponents of deliberation hope deliberative in stitutions will provide, such as higher quality policy, greater appreciation of the views of the opposing side, cultural pluralism and citizen welfare [243]. According to deliberative democrats, we must focus on why and how we come to adopt our views, and whether they can be defended in a complex social setting with people of opposed preferences. This will complement voting, the necessary mode of participation, by a “conscious con frontation of one’s own point of view with an opposing point of view, or of the multiplicity of diverse viewpoints that the citizen, upon reflection, is likely to discover within his or her own self"[248]. Under conditions of ideal deliberation, "no force except that of the 
3 
better argument is exercised"[149]. 
Information diversity is also an important concept in communication studies. The freedom of media, a multiplicity of opinions and the good of society are inextricably mixed [243]. Free Press theory, a theory of media diversity, states that we establish and preserve conditions that provide many alternative voices, regardless of intrinsic merit or truth, with the condition that they emerge from those whom society is supposed to benefit its individual members and constituent groups[338]. What is good for the mem bers of the society can only be discovered by the free expression of alternative goals and solutions to problems, often disseminated through media [243]. 
While many scholars from different disciplines agree that information diversity is an important value that we should include in the design of institutions, policies and online services, this value is often reduced to a single definition, such as “source diversity", or “hearing the opinion of the other side". In the next subsections, we explain that just having a deliberation is not enough, and a bias against arguments made by deliberators who are in the minority in terms of their interests in the decision being made can exist. 
3.3.2. DIMENSION OF INFORMATION DIVERSITY 
Following Napoli [1999], we may distinguish three different dimensions of diversity. The first dimension is source diversity, which is diversity in terms of outlets (cables and chan nel owners) or program producers (content owners). It is assumed that if source diversity increases, the second dimension content diversity will also increase. Content diversity consists of diversity in format (program-type), demographic (in terms of racial, ethnic, and gender), and idea-viewpoint (of social, political and cultural perspectives). The third dimension exposure diversity deals with audience reach and whether users have actually consumed the offered items diversely. 
In the US, with the “free marketplace of ideas" theory, it is assumed that increasing source diversity will increase content diversity and exposure diversity will follow these two. American media policy consequently focuses on source diversity by way of compe tition and antitrust regulation [336]. However, whether more media competition (more sources) really brings about more media variety is a highly debated question and re search addressing this relationship has not provided definitive evidence of a systematic relationship [175, 219, 243, 335]. Highly competitive media markets may still have low content diversity and media monopolies can produce highly diverse supply of media content [336]. It has also been argued that to fulfill the objectives of the marketplace of ideas metaphor, policymakers need to focus on exposure diversity. So, one should not look at availability of different sources or content, but whether the public consumes
42 3. VIEWPOINT DIVERSITY IN ONLINE SOCIAL NETWORKS - AN EMPIRICAL ANALYSIS 
them diversely [243]. 
3.3.3. MINORITIES AND OPENNESS 
According to van Cuilenburg [1999], media diversity has to be externally benchmarked in some way and should always be compared with relevant variations in society and so cial reality. Computer Science researchers often use popularity (i.e., trending topics) or locality (items posted by friends and friend of friends) to determine the importance of news items [57]. Some use number of available sources to measure diversity [15], mirror 3 
ing the “free marketplace" approach of diversity used in American media policy, which is based on the idea of competition and freedom of choice [336]. 
However, Karppinen [2009] argues that the aim of media diversity should not be the multiplication of genre, sources or markets, but giving voice to different members of the society. We should not see diversity as something that can be measured through the number of organizations or channels or just “having two parties reach all citizens". Karp pinen holds that we should focus on democratic distribution of communicative power in the public sphere and whether everyone has the chance and resources to get their voices heard. Karppinen argues: “the key task for media policy from the radical pluralist perspective is to support and enlarge the opportunities for structurally underprivileged actors and to create space for the critical voices and social perspectives excluded from the systematic structures of the market or state bureaucracy"[174]. If democratic pro cesses and public policies exclude and marginalize members of segregated groups from political influence to the extent that privileged groups often dominate the public policy process, they will magnify the harms of segregation. These “minorities" must be politi cally mobilized and included as equals in a process of discussing issues [369]. 
McQuail and van Cuilenburg [1983] propose to assess media diversity by introduc ing two normative frameworks. The norm of reflection checks whether "media content proportionally reflects differences in politics, religion, culture and social conditions in a more or less proportional way". The norm of openness checks whether media "pro vide perfectly equal access to their channels for all people and all ideas in society". If the population preferences were uniformly distributed over society, then satisfying the first condition (reflection) would also satisfy the second condition (equal access). How ever, this is seldom the case [335]. Often population preferences tend toward the middle and mainstreams. In such cases, the media will not satisfy the openness norm, and the preferences of the minorities will not reach a larger public. This is undesired, because "social change usually begins with minority views and movements (...) asymmetric me dia provision of content may challenge majority preferences and eventually may open up majority preferences for cultural change in one direction or another". [335]. Van Cuilenburg [1999] argues that the Internet has to be assessed in terms of its ability to give open access to new and creative ideas, opinions and knowledge that the old media do not cover yet. Otherwise it will only be "more of the same". 
3.4. POLARIZATION IN THE NETHERLANDS AND TURKEY Before discussing methods and the results of our empirical study that focused on Dutch and Turkish users, we give a short overview of political diversity for two countries and
3.4. POLARIZATION IN THE NETHERLANDS AND TURKEY 
43 
explain why they are interesting for a case study of information diversity. 3.4.1. THE NETHERLANDS 
Pillarization (Dutch: “verzuiling") is a process that occurred in the Netherlands and reached its highest point in 1950’s. During this period, several ideological groups mak ing up the Dutch society are systematically organized as parallel complexes that are mutually segregated and polarized [268, 347]. As part of this social apartheid dividing the population into subcultures, political parties were used for political mobilization of 3 
the ideologically and religiously defined groups and social activities were concentrated within the particular categorical group [303]. Few contact have existed between differ ent groups and internally the groups were tightly organized [204]. Elites at the ‘top’ level communicated, while the ones at the ‘bottom’ did not. Pillarization had an effect on parental choice of an elementary school for children, the voting for political parties and the choice on which daily newspaper to read [191]. People belonging to a pillar retreated into their own organizations and entered into a ‘voluntary’ isolation, because they per ceive that values important to them are threatened [218]. 
Depillarization (Dutch: “ontzuiling”) started in mid 1960’s as a democratization pro cess and pillarization has lost much of its significance since the 1960s as a result of sec ularization and individualization. Even though depillarization has started, many insti tutional legacies in present-day Netherlands still reflect its pillarized past, for example in its public broadcasting system or in the school system [349]. The Netherlands con tinues to be a country of minorities, which may be a main reason that consensus seems so ingrained in the Dutch political culture [263]. The Dutch parliament has 12 political parties. Due to the very low chance of any party gaining power alone, parties often form coalitions. 
Netherlands has created several media policies set afterwards to implement diver sity in the media. The Media Monitor, an independent institution, measures ownership concentration, editorial concentration and audience preferences [19]. It also measures diversity of television programming on the basis of a content classification system, by categorizing program output in categories like news and information, education, drama, sports, etc. [221, 336]. 
3.4.2. TURKEY 
Turkey has regularly held free and competitive elections since 1946. The country has alternated between a two-party political system and a multi-party system. Electoral pol itics has often been dominated by highly ideological rival parties and military inventions changed the political landscape several times [321]. Elections in 2002 led to a two-party parliament, partially due to a ten per cent threshold. The Justice and Development Party (AKP) won the elections and still is the ruling party, having an absolute majority. The parliament is currently formed by 4 political parties. While AKP has 59% of the MP’s, secular CHP has 24%. 
AKP’s dominance and the despair and sense of marginalization felt by its opponents threaten to create a political polarization along with fierce institutional clashes between the AKP government and the secular elites that retain a foothold in the military, the judi cial system, and other parts of the bureaucracy. Muftuler-Bas and Keyman [2012] argue
44 3. VIEWPOINT DIVERSITY IN ONLINE SOCIAL NETWORKS - AN EMPIRICAL ANALYSIS 
that “many other polarizing social and political struggles remain unresolved in Turkey, and mutually antagonistic groups remain unreconciled. This social and political polar ization remains potentially explosive and reduces the capacity for social consensus and political compromise". Similarly Unver [2011] claims that "the society is pushed towards two extremes that are independent of party politics. (...) Competing narratives and "re alities" clash with each other so intensely, that the resultant effect is one of alienation and "other-ness" within the society." 
3 
Some scholars argue that, the top-down imposition of concepts such as democracy, political parties and parliament as part of westernization efforts is causing the socio political polarization in Turkey [12]. Agirdir [2010] argues that "the system does not breed from the diverse interests and demands of the society, but around the values and interests of a party leader and the narrow crew around her". Economic voting behavior, religiosity, and modern versus traditional orientation seem to be the strongest drivers of polarization [367]. Some argue that, after 2011 polarization has increased and reached its highest points in Turkish history [253]. Report of research group KONDA indicate that the polarization mainly occurs between three groups: religious conservatives, tradition alist conservatives and moderns [6]. The difference of opinion between different clus ters about secularity, tolerance and political change issues in total contradiction of each other, therefore a danger of absolute social polarization is imminent [6]. Kiris [2011] ob serves an identity-based polarization, between secularists and islamists, between Turk ish nationalists and Kurdish Ethnic Nationalists, and between Alevis and Sunnis (differ ent sects of Islam). 
Turkish Radio Television Supreme Council (RTUK) was established in order to regu late the private broadcasting and to control the compliance of the broadcasts with the le gal framework. RTUK is granted with the authority of giving penalties (for breaching the legal framework) to the broadcasters, which may range from warning to the suspension of the TV and radio channels. RTUK, which is responsible for supervision of TV and radio programs in Turkey, consists of total twelve members. Three out of twelve are chosen and appointed by the President. RTUK makes sure that the constitutional language, which is Turkish, is used in programs and the elements of Turkish-Islamic ethics and Turkish Islamic world view will be given a significant place. Further, it control whether Turkish language, Turkish history, historical values, Turkish way of life, thoughts and feelings are given a significant place in broadcasting programs [1]. 
RTUK is sometimes referred as “the Censure Board" [236] and its decisions of pe nalizing the broadcasters so as to implement the Radio and Television law have been criticized domestically and internationally [30, 80]. RTUK played a crucial role in link ing legislation to implementation with respect to control of Kurd nationalist and Muslim religious broadcasts. It singled out Kurd nationalist and Muslim religious propaganda and enforced the most coercive penalties for this type of infraction. Almost all high in tervals of cease-broadcast days were applied to broadcasts that aired separatist and reli gious propaganda [80]. RTUK does not have a diversity policy and the lack of diversity in programme-making is said to undermine the quality of the audio-visual media [30].
3.5. METHOD 
3.4.3. CONCLUSION 
45 
In short, the Netherlands and Turkey are two different countries if we consider the polit ical landscape and diversity policy. The Dutch society is less polarized than it was half a century ago, while the Turkish society is thought to be heavily polarized. The Dutch Par liament contains many political parties, no party has absolute power to govern alone. Turkey, on the other hand has few political parties represented in the government and the ruling party has almost 60% of all the seats. Further, the Dutch media is regulated with a diversity policy. While Turkey has a similar institution, it acts more as a censor 3 
board and does not employ an active diversity policy. If the social networking platforms mirror the society, then we can expect the Dutch users to receive more diverse content, while the Turkish users to be more polarized and have a less diverse newsfeed. 
3.5. METHOD 
In this section we provide our method of data collection, present our research model and the metrics we have devised to measure information diversity. 
3.5.1. DATA COLLECTION 
In January 2013, over a period of more than one month we crawled microblogging data via the Twitter REST API1. We started from a seed set of Dutch and Turkish Twitter users Us, who mainly publish news-related tweets. We have selected different types of users including mainstream news media, journalists, individual bloggers and politicians. The list of these “influential" users were picked up from different ranking sites. For the Dutch ranking, we used Peerreach2, Twittergids3and Haagse Twitter-stoIp4. For Turkish rank ing, we used TwitterTurk5and TwitterTakip6. 
By monitoring the Twitter streams of Us, we were able to add another set of users Un , who followed and retweeted at least 5 items from users inUs. After removing users who were involved in spam, we had 1981 Dutch users and 1746 Turkish users. We mapped the political leaning of Dutch seed users into five groups and the political leaning of Turkish seed users into nine groups. We did this using a number of public data [52, 189, 323, 345]. The political stance in the landscape is determined by [30, 52, 190]. 
3.5.2. RESEARCH QUESTIONS 
The main question in this research is the following: “Does political culture affect in formation diversity in Twitter?”. To answer this question, we have provided some sub questions. 
1. Q1: Seed User Interaction Do seed users from one end of the political spectrum ever tweet links from another category? Do they reply to each other? The results 
1https://dev.twitter.com/docs/api/1.1 
2http://peerreach.com/lists/politics/nl 
3http://twittergids.nl/ 
4http://alleplanten.net/twitter/site/de-resultaten/belangrijke-personen/ 
5http://twitturk.com/twituser/users/turk 
6http://www.twittertakip.com/
46 3. VIEWPOINT DIVERSITY IN ONLINE SOCIAL NETWORKS - AN EMPIRICAL ANALYSIS 
of this question is relevant to the previously conducted studies that studied media bias on Twitter, such as Wei et al. [2013] 
2. Q2: Source Diversity Is the newsfeed of social media users diverse? Are they re ceiving updates from a diverse set of users? Does indirect exposure (e.g., via retweets or weak-links) increase diversity marginally? Result of these questions are relevant to the previously conducted studies, such as An et al’s [2011]. 
3. Q3: Exposure Diversity Do users share items from a diverse set of users or mainly 3 
from the same political category? This question is relevant to the framework pro vided by Napoli, which we have mentioned in Section 3.3.2. 
4. Q4: Openness Can minorities reach the social media users, so that “equal access" principle is satisfied? This question is relevant to the normative theory of [219] and [175], which we discussed in Section 3.3.3. 
5. Q5: Input-Output Correlation Do users post political messages whose political position reflects the political position of those messages that the users receive? Or do the messages they chose to retweet show a political position significantly skewed from the political position of the messages which they receive? Result of this question is relevant to the previously conducted studies such as Jurgens et al.’s [2011]. 
3.5.3. ENTROPY 
While translating the concepts introduced in the previous subsection into metrics, we apply the following entropy formula used by van Cuilenburg [2007] to measure tradi tional media diversity, which is based on the work of Shannon [1948] : 
−(Xpilogpi)/−(log(1/n)) (3.1) 
In [339], "pi " represents the proportion of items of content type category i. n rep resents number of content type categories. We use this formula for calculating source diversity and exposure diversity in our Twitter study. For instance in source diversity, "pi " represents incoming tweets from seed users with a specific political stance, while "n" represents all possible categories. As a result of this formula, the user will have a diversity between 0 and 1, where 0 represents minimum diversity and 1 represents max imum diversity. Figure 3.1 shows a user that receives equal amount of tweets from all political categories and has an incoming diversity of 1. He only retweets from one polit ical category, therefore he has an outgoing diversity of 0. 
3.5.4. TRANSLATING RESEARCH QUESTIONS INTO METRICS 
Source Diversity For each user we used Equation 3.1 to compare the tweets published by her direct followees (people he follows) from different groups of which the po litical leanings have been categorized as discussed above (See Figure 3.2). We then also added the tweets a user gets through retweets and investigated if the user re ceives more diverse information through indirect media exposure (See Figure 3.3).
3.5. METHOD 
47 
  
3 Figure 3.1: Applying entropy 
  
  

Figure 3.2: Direct source diver sity 
Figure 3.3: Indirect source diver sity 
  
  
Figure 3.4: Minority access Figure 3.5: Input-Output Correlation
48 3. VIEWPOINT DIVERSITY IN ONLINE SOCIAL NETWORKS - AN EMPIRICAL ANALYSIS 
Outgoing Diversity To measure what the user is sharing after he was exposed to dif ferent incoming information, we used Equation 3.1 to compare the retweets he makes for each political category. 
Openness For this definition of diversity, we first defined all seed users who belong to a political party that is either not represented in the parliament, or is represented with few MP’s. We also included MP’s of a large political party who belong to an ethnic minority. That makes for instance the Kurdish Party BDP and its MP’s a minority in Turkey, while we consider the Greens as a minority in the Netherlands. 
3 
See Appendix A for a list of minorities. Both users defined as minorities create about 15% of the all observed tweets for both countries. 
We then looked whether the user is receiving minority tweets directly or indirectly (See Figure 3.4). We defined two metrics to measure minority access. We first look at the ratio of minority tweets a user gets out of all minority tweets: 
# received minority tweets 
# all published minority tweets(3.2) 
We later calculate the ratio of minority tweets in a users’ timeline 
# received minority tweets 
#received tweets from seeds(3.3) 
Input-Output Correlation For each user in our sample we look whether the maximum number of the political position of the messages retweeted by a user is significantly skewed from the political position of the messages that she receives. 
max(incoming political category) == max(outgoing political category) (3.4) 
For instance, Figure 3.5 shows a biased user which receives most items from category 1, and also retweets mainly from category 1. 
3.6. RESULTS 
This section shows the results for the defined metrics. We tested statistical significance of our results with a two-tailed t-Test where the significance level was set to α = 0.01 unless otherwise noted. 
3.6.1. DISTRIBUTION OF SEED USERS AND THEIR FOLLOWERS 
Figures 3.6 and 3.7 show the distribution of the seed users for both countries. Figures 3.8 and 3.9 show the distribution of regular users. We see that our selection of popular users covers the political spectrum and it is not concentrated on a single political category. We have used several sources to do the seed user categorization [30, 189, 189, 190, 323, 345]. We used the retweet behavior of the users to assign them to a political category to identify their political stance.
3.6. RESULTS 
49 
  
  
3 Figure 3.6: Dutch seed user distribution Figure 3.7: Turkish seed user distribution 
  
  
Figure 3.8: Dutch user distribution Figure 3.9: Turkish user distribution 
3.6.2. SEED USER BIAS 
Tables 3.1a and 3.1b show the retweet and reply behavior of seed users. Each row shows the category of users who retweet an item or reply to another user. The columns show the source of their retweet or the user they interact with. We observe that 73% of the left seed users retweet left items and reply to left users, while 72% of the right users do the same. The situation is more extreme for Turkish seed users: 93% of left seed users retweet from and reply to left, while 94% of the right seed users show the same behavior. 
3.6.3. SOURCE AND OUTPUT DIVERSITY 
Table 3.2a shows the results for research questions Q2 and Q3. Here we see that on a scale of 0 to 1, the diversity of the incoming tweets for an average user is approximately 0.6 and the results are not very different for both countries. While diversity is not perfect, we cannot really observe a true cyberbalkanization and we cannot observe a significant dif ference between two countries. We observe that indirect communication (retweets) does increase diversity, but not dramatically. Figure 3.10 and Figure 3.11 show the distribution of source diversity among users. We observe that, indirect communication decreases the number of polarized users who have a diversity approaching 0 for both countries. Ap proximately 27% of the Dutch and 29% of the Turkish users have an indirect diversity under 0.5. However, if we look at the diversity of an average user’s output, we see much lower numbers. It is approximately 0.4 for both countries. Figure 3.12 shows the distri bution of output diversity among the population. About 52% of the Dutch and 66% of the Turkish users have an output diversity lower than 0.5. We do not observe a big difference
50 3. VIEWPOINT DIVERSITY IN ONLINE SOCIAL NETWORKS - AN EMPIRICAL ANALYSIS 
Table 3.1: Seed user bias 
(a) Netherlands(b) Turkey 
3 
User / Source 
	Left 
	Right
	Left 
	73% 
	27%
	Right 
	72% 
	28%
	



Table 3.2: Different dimensions of diversity (a) Source Diversity (on a scale of 
User / Source 
	Left 
	Right
	Left 
	93% 
	7%
	Right 
	94% 
	6%
	



0 to 1)(b) Input-Output Correlation 




	NL 
	TR
	Direct 
	0.63 
	0.58
	Indirect 
	0.68 
	0.62
	Outgoing 
	0.43 
	0.40
	



(c) Minority Access 


	NL 
	TR
	# users 
	657 
	828
	% users 
	33% 
	47%
	







	NL 
	TR
	minority reach 
	15% 
	2%
	minority exposure 
	23% 
	2%
	% users under <0.05 reach 
	14% 
	57%
	% users under <0.05 exposure 
	23% 
	55%
	



between different countries. 
3.6.4. MINORITY ACCESS 
Table 3.2c shows the results for the research question Q4. First row, which we call “minor ity reach" shows the result for Equation 3.2 and the second row, which we call “minority exposure" shows the result for Equation 3.3. We observe that an average Dutch Twitter user will receive 15% of the produced minority tweets, whereas an average Turkish user will only receive 2% of them. Later, we observe that minority tweets make up 23% of an average Dutch users’ incoming tweets from seed users, while it only makes up 2% for a Turkish user. Figures 3.13 and 3.14 show the distribution of users for this metric. Here we observe a significant difference between two countries. About 55% of the Turkish users have a minority exposure under 0.05 and 57% of them have a minority reach under 0.05. The percentages are much lower for the Dutch users: 14% and 23% respectively. 
3.6.5. INPUT OUTPUT CORRELATION 
Table 3.2b shows the results for the research question Q5. The first row shows the num ber of “biased" users. These are users whose output correlates with their input. Such users make up 33% of the Dutch and 47% of the Turkish userbase. Further, if we only consider a bias towards a certain political category that is higher than 15% (for both in put and output), 26% of the Dutch and 36% of the Turkish users show this behavior.
3.6. RESULTS 
1 
1 
Direct Input EntropyDutch 
51 
Dutch 
0.8 
0.8 
Turkish 
Input Entropy 
0.6 
0.6 
0.4 
0.4 
0.2 
0.2 
0 
0 
0% 25% 50% 75% 100% Users 
Turkish 
0% 25% 50% 75% 100% Users 
3 
Figure 3.10: Direct source diversity 
1 
0.8 
Ouput Entropy 
0.6 
0.4 
0.2 
0 
Figure 3.11: Indirect source diversity 
Dutch 
Turkish 
0% 25% 50% 75% 100% 
Users 
Figure 3.12: Output diversity 
1 
1 
Minority/all minority 
Dutch 
0.8 
0.8 
Turkish 
Minority/All 
0.6 
0.6 
0.4 
0.4 
0.2 
0.2 
0 
0 
0% 25% 50% 75% 100% Users 
Dutch 
Turkish 
0% 25% 50% 75% 100% Users 
Figure 3.13: Minority Reach 
Figure 3.14: Minority Exposure
52 3. VIEWPOINT DIVERSITY IN ONLINE SOCIAL NETWORKS - AN EMPIRICAL ANALYSIS 
3.7. LIMITATIONS 
This study has several limitations. First of all, next to the accounts of traditional media outlets on Twitter, we also selected politicians and bloggers. While they mainly tweet political matters, it is possible that they have shared personal and non political matters as well. 
Second, while the results give us an idea on the political landscape of the studied countries, Twitter does not represent ‘all people’. As boyd and Crawford [2011] have stated, “many journalists and researchers refer to ‘people’ and ‘Twitter users’ as syn 3 
onymous (...) Some users have multiple accounts. Some accounts are used by multi ple people. Some people never establish an account, and simply access Twitter via the web”. Therefore we cannot conclude that our sample represent the real population of the studied countries. 
Third, input-output correlation does not always implicate that the volume of the content affects the items users share. Users might already be biased before they select their sources and can therefore follow more from certain sources and share from certain categories. 
Fourth, users will make different uses of Twitter. Some might use it as its primary news source, therefore following mainstream items, while others will use it to be in formed of the opposing political view or to find items missing in the traditional media. Therefore, we do not know why some users only follow sources from a specific political category. More qualitative studies are needed. 
3.8. DISCUSSION 
In this study we have shown different dimensions of diversity and discussed another di mension, namely minority access. This dimension is often missing in the research per formed by computer scientists. However, as many communication scholars and philoso phers have argued, while the media should reflect the preferences present in the society, it should also allow equal access to everyone, including those whose common social lo cation tends to exclude them from political participation. Public life needs to include differently situated voices to be able to articulate their concerns and interests, not just the ones who are in majority. 
We have shown that different definitions of diversity can introduce different metrics and the question whether “the filter bubble exists" will have different answers depend ing on the metric and culture. For instance, according to the results of our study, source diversity does not differ much for Turkish and Dutch users and we certainly cannot ob serve a bubble. However, if we consider output, then we see that the diversity is much lower. Further, if we consider the minority access as a diversity metric, we see that mi norities cannot reach a large percentage of the Turkish population. 
In the abundance of digital information and filters to deal with information overload, ideas and opinions of minorities should not be lost. Design choices in software codes and other forms of information politics still largely determine the way information is made available and who can speak to whom under what condition [174]. According to Karppinen [2009], it is important to make decisions about standards, because those "can have lasting influence on media pluralism, even if they are not necessarily recognized as
3.8. DISCUSSION 
53 
sites of media policy as such". However, making minority voices reach a wider public is no easy matter. While identifying minorities and their valuable tweets is no easy task, showing these items to “challenge averse" users is a real challenge [238]. For instance Munson et al.[2013] provided people with feedback about the political lean of their read ing behaviors and found that such feedback had only a small effect on nudging people to read more diversely. More research is needed to understand how users’ reading behavior change and to determine the conditions that would allow such a change. 
In the recent months, Turkey experienced several political protests that spontaneously erupted against the destruction of trees and the building of a shopping mall at Gezi Park 
3 
in Taksim Square and large scale corruptions within the government. Twitter and Face book played a vital role during these movements and became the only communication medium when traditional media performed self-censorship [89, 152, 251]. It would be very useful to see whether the political stance of our observed users have changed. It is also challenging to identify the opinion leaders during these movements and find whether they communicate with each other or form their own “bubbles". It is further valuable to see if minorities were able to reach a wider public during those protests. A hashtag based political communication and diversity analysis could bring new insights. 
Our study was focused on Twitter and studied whether users have put themselves in bubbles by following individuals from only one end of the political spectrum and showed a biased sharing behavior. Twitter itself does not employ a personalization algorithm in a user’s timeline. However other social networking platforms, such as Facebook, do use a personalization algorithm and filter certain information on user’s behalf [45]. Future studies can perform black-box testing techniques to determine whether filters used by these platforms lead to bubbles (See [172]). Creating multiple profiles while modifying certain factors, such as political affiliation, age, location, etc. can help us detect bubbles, if they exist. 
APPENDIX A: LIST OF MINORITIES 
Dutch minorities: Keklik Yucel, SGP, Khadija Arib, Vera Bergkamp, Sadet Karabulut, Far shad Bashir, Tanja Jadnanansing, Piratenpartij NLD, Partij van de Dieren, Fatma Koser Kaya, ChristenUnie, Groenlinks, Marianne Thieme, Femke Halsema. 
Turkish minorities: Ayca Soylemez, Evrensel, Aydinlik, Ozgur Gundem, Pinar Ogunc, Bianet, Sebahat Tuncel, Sol Haber Portali, Halkin Gazetesi Birgun Yildirim Turker, Ufuk Uras, Selahattin Demirtas, Sirri Sureyya Onder, Sinan Ogan, Hasip Kaplan. 
Note that both minorities create about 15% of all tweets produced by seed users.
4 
DEMOCRACY, FILTER BUBBLE AND DESIGN 
The overriding question, ‘What might we build tomorrow?’ blinds us to questions of our ongoing responsibilities for what we built yesterday. 
Paul Dourish and Scott Mainwaring 
4.1. INTRODUCTION 
Cyberbalkanization refers to the idea of segregation of the Internet into small political groups with similar perspectives to a degree that they show a narrow-minded approach to those with contradictory views. For instance Sunstein [2007] argued that thanks to the Internet, people could join into groups that share their own views and values, and cut themselves off from any information that might challenge their beliefs. This, according to Sunstein, will have a negative effect on the democratic dialogue. Recently others have argued that personalization algorithms used by online services such as Facebook and Google display users similar perspectives and ideas and remove opposing viewpoints on behalf of the users without their consent [256]. According to Pariser [2011], users might get different search results for the same keyword and those with the same friend lists can receive different updates. This is because information can be prioritized, fil tered and hidden depending on a user’s previous interaction with the system and other factors [45, 83]. This might lead to the situation in which the user receives biased infor mation. In case of political information, it might lead to the situation that the user never sees contrasting viewpoints on a political or moral issue. Users will be placed in a “filter bubble” and they will not even know what they are missing [256]. As a consequence, the epistemic quality of information and diversity of perspectives will suffer and the civic discourse will be eroded. 
This chapter has been accepted for publication in Ethics and Information Technology as “Bozdag E. and van den Hoven J. Breaking the Filter Bubble: Democracy and Design” 
55
56 4. DEMOCRACY, FILTER BUBBLE AND DESIGN 
After Pariser’s book has been published, the danger of filter bubbles received wide attention in the media, in academia and in industry. Empirical studies have been con ducted to confirm or to debunk its existence. While algorithms and online platforms in general have been criticized because they cause filter bubbles, some designers have developed algorithms and tools to actually combat those bubbles. However, as we will show in this paper, the methods and goals of these tools differ fundamentally. Some try to give users full control and allow them to even increase their bubble. Some modify users’ search results for viewpoint diversity without notifying the user. This is because the filter bubble has become a term that encompasses various criticisms. These criti cisms differ because democracy is essentially a contested concept and different democ racy models require different norms. As this paper will show, some will criticize the filter bubble due to its negative effect on user autonomy and choice, while others emphasize the diminishing quality of information and deliberation. In this paper we will show that 
4 
while there are many different democracy theories, only the diversity related norms of a few of them are implemented in the tools that are designed to fight filter bubbles. We will also show that some norms (e.g., the inclusion of minorities in the public debate) are completely missing. We will argue that if we want to fully use the potential of the In ternet to support democracy, all these diversity related norms should be discussed and designed, and not just the popular or most dominant ones. 
In this paper, we first provide different models of democracy and discuss why the filter bubble poses a problem for these different models. Next, we provide a list of tools and algorithms that designers have developed in order to fight filter bubbles. We will do this by discussing the benchmarks these tools use and the democracy model the tools exemplify. We will show that not all relevant democracy models are represented in the overview of available diversity enhancing tools. Finally, we discuss our findings and pro vide some recommendations for future work. 
4.2. DEMOCRACY: DIFFERENT THEORIES, DIFFERENT BENCH MARKS 
Democracy refers very roughly to “a method of group decision making characterized by equality among the participants at an essential stage of the collective decision making” [60]. While some models of democracy emphasize the autonomy and individual pref erences of those who take part in this collective decision making, others highlight the inclusion of free and equal citizens in the political community and the independence of a public sphere that operates as a middle layer between state and society [150]. Some emphasize the need of an informed (online) debate and the epistemic quality of infor mation before decisions are made [153]. Others point out the need to increase the reach of minorities and other marginalized groups in the public debate [369]. 
While the filter bubble has been a concern for many, there are different answers to the question as to why filter bubbles are a problem for our democracy. The answer one gives to the question depends on one’s understanding of the nature and value of democ racy, on one’s conception of democracy. Different democracy theories exist and they have different normative implications and informational requirements.. A tool that im plements one particular norm will be quite different in its form and goals than another
4.2. DEMOCRACY: DIFFERENT THEORIES, DIFFERENT BENCHMARKS 
57 
tool which implements a different norm. Before we provide examples of different tools, we will provide a framework of some basic conceptions of democracy and the relevant norms for each model. 
4.2.1. LIBERAL VIEW OF DEMOCRACY 
The classical liberal view of democracy attempts to uphold the values of freedom of choice, reason, and freedom from tyranny, absolutism and religious intolerance [91, 158]. Liberalism started as a way to challenge the powers of “despotic monarchs” and the church. Once liberalism achieved victory over these old “absolute powers”, many lib eral thinkers, began to express fear about “the rising power of the demos” [158, 213, 226]. They were concerned by the new dangers to liberty posed by majority rule against mi norities and the risk of the majority tyrannizing over itself, leading to a need for people 4 
to ‘limit their power over themselves’. 
Bentham [1780] argues that, since those who govern will not act the same way as the governed, government must always be accountable to an electorate called upon fre quently and that electorate should be able to decide whether their objectives have been met. Next to voting, ‘competition’ between potential political representatives, ‘separa tion of powers’, ‘freedom of the media’, ’speech and public association’ should be en sured to sustain ‘the interest of the community in general’ [37]. Individuals must be able to pursue their interests and goals without the risk of arbitrary political interference from the governing bodies, to participate freely in economic transactions, to exchange labor and goods on the market and to appropriate resources privately [158]. 
The liberal view of democracy is often criticized, because it construes democracy as an aggregation of individual preferences through a contest (in the form of voting), so that the preferences of the majority win the policy battle. However, this model has no way of distinguishing normatively legitimate outcomes from the preferences and the desires of the powerful, and makes no distinction between purely subjective preferences and legitimate and shared (quasi objective) judgments [64, 65, 369]. 
Filter bubbles are a problem according to the liberal view, because the non-transparent filters employed by online algorithms limit the freedom of choice. In addition, the liberal view states that citizens must be aware of different opinions and options, in order to make a reasonable decision. A filter imposed on a user –unbeknownst to them - will vi olate their autonomy, as it will interfere with their ability to choose freely, and to be the judge of their own interests. Further, the principle of separation of powers and the free dom of the media can also be in danger, if the algorithms are designed in such a manner as to serve the interests of certain individuals or groups. Finally, filters might damage the “liberty of thought”. Liberty of thought, discussion and action are the necessary condi tions for the development of independence of mind and autonomous judgment. Liberty of thought creates reason and rationality, and in turn the cultivation of reason stimulates and sustains liberty. If one is ‘coerced’ by the filters, reason will also diminish. While some thinkers such as Mill [1859] also emphasize the diversity of opinion, most liberal thinkers do not mention this as a requirement. Liberal citizens must be ‘potentially’ in formed so that the elected act accountably, but deliberation according to the liberal view is not necessary. Loss of autonomy caused by filters seems to be the main issue, accord ing to the liberal view, while diversity of opinions and perspectives is not a concern.
58 4. DEMOCRACY, FILTER BUBBLE AND DESIGN 
4.2.2. DELIBERATIVE DEMOCRACY 
Elster [1997] characterizes deliberative democracy as “decision making by discussion among free and equal citizens”. Deliberative democrats propose that citizens address societal problems and matters of public concern by reasoning together about how to best solve them. This can be made possible by deliberative procedures, which help to reach a moral consensus that satisfies both rationality (defense of liberal rights) and le gitimacy (as represented by popular sovereignty) [148]. Individuals participating in the democratic process can change their minds and preferences as a result of reflection. Ac cording to Cohen [2009], deliberative democracy can be seen (1) as a matter of forming a public opinion through open public discussion and translating that opinion into le gitimate law; (2) as a way to ensure elections are themselves infused with information and reasoning; (3) as a way to bring reasoning by citizens directly to bear on addressing 4 
regulatory issues. In all cases the goal is to use the common reason of equal citizens who are affected by decisions, policies or laws, instead of having them enter into bargaining processes or represent them by means of the aggregation of their individual preferences. Democracy, no matter how fair, no matter how informed, no matter how participatory, does not qualify as deliberative unless reasoning is central to the process of collective decision making. 
There are different versions of deliberative democracy [332]. Rawls’ [1971, 1997] con ception of deliberation is based on the idea of public reason, which is defined as “the basic moral and political values that are to determine a constitutional democratic gov ernment’s relation to its citizens and their relation to one another”. By means of public deliberation, people settle their disputes with respect and mutual recognition towards each other. Habermas [1998] provides similar conditions in his “ideal speech situation”. The Rawlsian approach aims at ‘accommodation’ of differences in a pluralistic society without criticizing people’s fundamental views of life, their so-called ‘comprehensive doctrines’ or ‘bringing them into deliberative discussion’. Habermas’ approach does the opposite, by also making moral or philosophical ideas and ideals part of the deliberative challenge. Both Rawls and Habermas advocate a ‘rational consensus’ rather than ‘mere agreement’ in political deliberation. For this purpose, Rawls uses the term ‘reasonable’, and Habermas introduces the notion of ‘communicative rationality’. 
Deliberative democrats argue that deliberation 1) enlarges the pools of ideas and in formation [65]. 2) helps us discover truths [216, 317]. 3) can lead us to a better grasp of facts [153]. 4) can lead us to discover diverse perspectives, practical stances towards the social world that are informed by experiences that agents have [41]. 5) can help us discover the seriousness of our disagreements and discover that there is a disagreement after all [63]. 6) can lead to a consensus on the “better or more reasonable” solution [195]. 7) promotes justice, as it requires full information and equal standing 8) leads to better epistemic justification and legitimacy than simply voting [153]. This is because political decisions based on deliberation are not simply a product of power and interest. It involves public reasons to justify decisions, policies or laws. 9) leads to better argu ments, since a citizen has to defend his proposals with reasons that are capable of being acknowledged as such by others [65]. 10) allows citizens to reflect on their own argu ments, that will lead to self-discovery and refined arguments [63]. 11) promotes respect, as it requires people to consider the opinions of others, despite fundamental differences
4.2. DEMOCRACY: DIFFERENT THEORIES, DIFFERENT BENCHMARKS 
59 
of outlook [153]. 
Critics of deliberative democracy argue that full fledged deliberation is difficult to at tain because 1) there is inequality in deliberative capabilities of citizens, which gives ad vantages to the rhetorically gifted and those who possess cultural capital and argumen tative confidence in leading the discussions [7]. 2) there is widespread incompetence and political ignorance among masses [7]. 3) voters are not interested in the common good, but only in self-interests [51]. 4) people are biased and may hold beliefs without investigation. Majority rule will amplify these mistakes and make democratic decisions worse [51]. 5) While participation of citizens is possible in small nations, vast numbers of people will inevitably entail deterioration of participation [158]. Past a certain thresh old, deliberation turns into a chaotic mess [195]. 6) Most citizens cannot spend the time to master the issues well enough to take meaningful stands on major issues. The infor 4 
mation processing cost and transaction cost is too high [343]. 7) Deliberation among like-minded users can cause polarization. When people deliberate on a relatively ho mogenous argument pool, they consolidate fairly easily, which is bad for outsiders. Evi dence from social psychology suggests that it is the viewpoints of the majority, not of the informed minorities, that can be expected to drive the relevant group judgments [7]. The informed minorities may refrain from disclosing what they know due to social pressure and be reluctant to dissent, thus not submitting the information to deliberation [314]. 8) Forcing participants to deliberation with limiting their arguments due to commonly shared rational premises, public reason or common good will prevent dissenting voices to share their perspectives and identities on their own terms [369]. 
Filter bubbles are a problem for deliberative democrats, mainly because of the low quality of information and the diminishing of information diversity. If bubbles exist, the pool of available information and ideas will be less diverse and discovering new perspec tives, ideas or facts will be more difficult. If we only get to see the things we already agree with on the Internet, discovering disagreement and the unknown will be quite difficult, considering the increasing popularity of the Internet and social media as a source of po litical information and news [230]. Our arguments will not be refined, as they are not challenged by opposing viewpoints. We will not contest our own ideas and viewpoints and as a result, only receive confirming information. This will lead us not to be aware of disagreements. As a consequence, the quality of arguments and information and respect toward one other will suffer. 
4.2.3. REPUBLICANISM AND CONTESTATORY DEMOCRACY 
In contemporary political theory and philosophy, republicanism focuses on political lib erty, understood as non-domination or independence from arbitrary power. The repub lican conception of political liberty defines freedom as a “sort of structural indepen dence, the condition of not being subject to the arbitrary or uncontrolled power” [264]. Pettit [1999] argues that people are free to the extent that no other group has “the capac ity to interfere in their affairs on an arbitrary basis”. To ensure that, according to Pettit [1999], there must be an “active, concerned citizenry who invigilate the exercise of gov ernment power, challenge its abuses and seek office where necessary”. In this theory, freedom as non-domination supports a conception of democracy where contestability takes the place usually given to consent. The most important implication is not that the
60 4. DEMOCRACY, FILTER BUBBLE AND DESIGN 
government does what the people want, but that people can always contest whatever decision the government has taken. While the republican tradition does not overlook the importance of democratic participation, the primary focus is clearly on avoiding the evils associated with interference and oppression. 
Pettit [1999] argues that the media has a major role in forming the public opinion, ensuring non-domination and the possibility of effective contestation. However, Pettit argues, the media often fail badly in performing these roles. According to Pettit, at every site of decision-making (legislative, administrative and judicial), there must be proce dures in place to “identify and display the considerations relevant to the decision”. The citizens should be able to contest these decisions if they find that the considerations did not actually determine the outcome. The decisions must be made “under transparency, 4 
under threat of scrutiny, and under freedom of information”. A group, even if they are a minority, should be able to voice contestation and must be able to speak out in a way that is liable to affect the proposed legislation. They must be able to contest in an effective manner, and they must be able to make themselves heard in decision-making quarters. To provide this, there must be reliable channels of publicity and information in place, so that the performance of the governing parties is systematically brought to attention. 
If we apply these norms to the design of online platforms, we can argue that online information platforms 1) must make the right information available to the citizens and should allow them to track when something important or relevant happens. In this way, citizens can become aware of possible oppression and can become active when they feel there is a need to. This can for instance be achieved by human curation that aims at in cluding important event that might affect the whole of society, in the information diet of everyone. It can also be achieved by means of personalization, so that, an event that is particularly important for a user can be highlighted for that user. 2) provide effective methods of contestation, so that citizens can make themselves heard with their contes tations and affect the proposed legislation or policy. This means that people should not only be able to contest, but also that the contestation should reach a large public so that it can result in an effective and inclusive discussion. 
Filter bubbles are a problem for advocates of contestatory democracy, because they interfere with realization of both conditions mentioned above. Bubbles both block the incoming and outgoing information channels. In order to raise critical questions, one must be aware of something that is a candidate for contestation. Someone cannot protest if they do not know that things relevant to them are happening. A filter bubble can block the reliable channels of publicity and information and may increase the risk that citi zens are unaware of important news. Filter bubbles prevent awareness of both the items that people could disagree with and the information on the basis of which they could justify their reasons for disagreeing. Furthermore it may turn out to be much more dif ficult to communicate and share ideas with potentially like minded others outside your filter bubble. For not every post or comment on Facebook will reach your followers and a website with key information might never make it to the top of one’s Google’s search results.
4.2. DEMOCRACY: DIFFERENT THEORIES, DIFFERENT BENCHMARKS 
61 
4.2.4. AGONISM / INCLUSIVE POLITICAL COMMUNICATION 
While most deliberative democracy models aims for consensus concerning a ‘common interest’, agonists see politics as a realm of conflict and competition and argue that dis agreement is inevitable even in a well-structured deliberative democratic setting, and even if the ideal of consensus regulates meaningful dialogues [233]. According to these critics, different and irreconcilable views will coexist and such an overlapping final con sensus can never be achieved. Having consensus as the main goal and the refusal of a vibrant clash of democratic but opposing political positions will lead to “apathy and disaffection with political participation” [232, 369]. According to Mouffe [2009], the aim of democratic politics according to advocates of this agonistic conception of democracy should not be seen as overcoming conflict and reaching consensus, because such a con sensus would actually be a consensus of the hegemony. 
4 
The aim of ‘agonistic pluralism’ then, is to construct the opposing viewpoints in such a way that it is no longer perceived as an enemy to be destroyed, but as an ‘adversary’. Thus, conflict must be in center stage in politics and it must only be contained by demo cratic limits. An adversary is “somebody whose ideas we combat but whose right to de fend those ideas we do not put into question” [233]. The difference with “deliberative democracy” is that ‘agonistic pluralism’ does not eliminate passions from the sphere of the public, in order to reach a consensus, but mobilizes those passions towards demo cratic designs. Democracy should then be designed so that conflict is accommodated and unequal power relations and hegemony in the society is revealed [87]. 
Mouffe [232] argues that although the advocates of deliberative democracy claim to address pluralism and the complexity of the society, their reference to reason and ratio nality tends to exclude certain groups from the political arena; therefore, they are essen tially not pluralistic. Similarly, Young [369] argues that if consensus becomes the ulti mate goal, some difficult issues or issues that only concern a minority might be removed from discussion for the sake of agreement and preservation of the common good [369]. The idea of a generalized and impartial public interest that transcends all difference, diversity and division is problematic, because the participants in a political discussion most likely differ in social position or culture. Our democracies contain structural in equalities (e.g., wealth, social and economic power, access to knowledge, status). Some groups have greater material privilege than others, or there might be socially or econom ically weak minorities. Therefore in such settings “the common good” is likely to express the interests and perspectives of the dominant groups [369]. The perspectives and de mands of the less privileged may be asked to be put aside for the sake of a common good whose definition is biased against them. 
Young [2002] argues that when there are structural conflicts of interest which gen erate deep conflicts of interest, processes of political communication are more about struggle than about agreement. However, according to Young, the field of struggle is not equal; some groups and sectors are often at a disadvantage. Fair, open, and inclu sive democratic processes should then attend to such disadvantages and institutionalize compensatory measures for exclusion. Democratic institutions and practices must take measures explicitly to include the representation of social groups, relatively small mi norities, or socially or economically disadvantaged ones. Disorderly, disruptive, annoy ing, or distracting means of communication are often necessary or effective elements in
62 4. DEMOCRACY, FILTER BUBBLE AND DESIGN 
such efforts to engage others in debate over issues and outcomes. Cristiano [2006] ar gues that due to cultural differences in society, deep cognitive biases make individuals fallible in understanding their own and other’s interests and compare the importance of others’ interest with their own. By default, people will fail to realize equal advancement of interests in society. Thus, special measures must be taken to make sure that equality is satisfied. 
Filter bubbles are a problem for agonists and supporters of inclusive political com munication, because they hide or remove channels through which opposing viewpoints can clash vibrantly. Minorities, and those who are disadvantaged due to structural in equalities need special exposure to be able to reach out with their voice to larger publics. However, filters that show us what we already agree with usually do not include such mi nority voices. If filters only show us what they consider “relevant” for us, then, the only 
4 
way to reach a large public will be through advertisements or by gaming the filters. This will violate the inclusion norm of modern democracies, as only the wealthy who can af ford such advertisements, or technologically advanced minds who can use algorithms to their own advantage or can game the algorithms of others and override them, will be able to express themselves. 
4.2.5. CONCLUSION 
Table 4.1 summarizes the democracy models we have introduced, the benchmarks they require, the points of critique they imply concerning the phenomenon of Filter Bubble. Liberal democrats stress the importance of self-determination, awareness, being able to make choices and respect for individuals. Filter bubbles are a problem for the liberal democrats especially due to restrictions on individual liberty, restrictions on choice and the increase in unawareness. Deliberative democracy attempts to increase information quality, discover the truth, discover facts, discover perspectives and discover disagree ments. This in the end leads to better epistemic justifications, better arguments and it increases legitimacy and respect towards one other. The filter bubble, according to deliberative democrats, hurts the civic discourse, mutual understanding and sensemak ing. Contestatory democracy on the other hand focuses on channels that allow citizens to be able to contest effectively, if there is a need. It does not aim for deliberation, but it requires citizens to have key information on important issues, and be aware of the op pressors. In contestatory democracy, the media should thus provide reliable channels of publicity, so that the performance of the governing parties is systematically brought to attention and can be contested. The filter bubble is a problem for contestatory democ racy, because it removes the reliable channels so that key information on both topics and grounds of contestation cannot be sent and received. Agonists criticize the consensus goal of deliberative democrats and argue that other norms such as inclusion should also be the goal of democracy. They argue that special attention must be paid to the voice of minorities and other disadvantaged members of society and by making sure that dissent is continuously present. The filter bubble is a problem for agonists, because it will si lence radical voices, will only reflect the viewpoints and perspectives of the mainstream and it will change agonism to antagonism.
4.2. DEMOCRACY: DIFFERENT THEORIES, DIFFERENT BENCHMARKS 
63 
Model of Democracy Norms Criticism of the Filter Bubble
Liberal 
	Awareness of availablepreferences 
Self-determination 
Autonomy 
Free media 
Adaptive preferences 
Respect human dignity
	User is unaware of theavailability of options 
User is restrained and individual liberty is cur tailed. 
The media is not free, it serves the interests of cer tain parties. 
Powers are not separated (advertiser and the infor mation provider are the same)
	Deliberative 
	Discover facts, perspec tives and disagreements 
Determine common in terests 
Construct identity by self discovery 
Refine arguments and provide better epistemic justifications 
Consensus 
Respect towards each other’s opinions 
A collective spirit 
Free and equal partici pants 
Rationality
	Epistemic quality of infor mation suffers 
Civic discourse is under mined 
No need to have better epistemic justifications. 
Respect for other opinions is decreased. 
Legitimacy is more diffi cult to achieve. There is a loss of a sense of an infor mational commons 
Communication suf fers as gaining mutual understanding and sense making is undermined
	



4 
64 4. DEMOCRACY, FILTER BUBBLE AND DESIGN 
4 
Republican and Contes tatory 
Freedom from domina tion by oppressors 
Contest matters effec tively 
Be aware of the oppres sors 
Diminishes one’s ability to contest. 
Diminishes one’s aware ness of the oppressors and their potentially manipu lative interventions 


Agonistic / Inclusive Po litical Communication
	Conflict rather than con sensus 
Passions rather than ratio nality 
Struggle rather than agreement 
Inclusion
	The adversary becomesthe enemy 
The minorities are ex cluded from the demo cratic process, their voices are lost
	



Table 4.1: Models of Democracy and Design criteria
4.3. SOFTWARE DESIGN TO COMBAT FILTER BUBBLES 
65 
4.3. SOFTWARE DESIGN TO COMBAT FILTER BUBBLES 
Many activists, including Eli Pariser [2011] have suggested to users that they should sab otage personalization systems by erasing web history, deleting cookies, using incognito option, trying other search engines and fooling the personalization system either by en tering fake queries or liking everything ever produced by your friends. However, these options are not only tedious, but they are bad for the user as well. As we will show in this section, personalization algorithms and other tools can actually also be designed and used to broaden a user’s worldview. 
As we have seen in Section 2, while filter bubbles should be seen as worrying devel opments in the digital world from the point of view of democracy, different conceptions and models of democracy point to different undesired consequences of such bubbles, ranging from loss of autonomy to the diminishing epistemic quality of information. In 4 
recent years, various tools have been developed by computer scientists either in the in dustry or in academia to fight filter bubbles. However, as designers hold different values and are assuming different models of democracy model either implicitly or explicitly, the tools they develop will reflect those values and democracy models. As Friedman [2006] argues, technology is not neutral and the values and biases that designers hold will man ifest themselves in the end product. 
In order to identify the state of the art tools and designs and analyze which criteria and methods they employ, we have created a carefully curated list. To come up with this list, between January 2014 and June 2014, we have performed the following inquiries: 1) we have checked the academic articles that cite Munson and Resnick [2010], one of the first papers that designed an experiment and created a tool to fight the filter bubble, in the HCI community. 2) we have frequently followed HCI researchers on Twitter and included the tools/experiments they have mentioned on filter bubble. 3) We have used Google search engine with specific keywords to find non-academic tools, including filter bubble, “design”, “selective exposure”. This gave us in total 15 tools/designs. 
In this section, we will show that, the different interpretations of the phenomenon filter bubble have led to different designs, tools and empirical studies. These tools dif fer in their goals ranging from personal fulfillment and development of cultural taste to promotion of tolerance and intercultural understanding. We will show that, some of the tools even allow the user to increase filter bubbles. The tools also differ in their methods, ranging from modifying users’ newsfeeds/search results without their notice to visualiz ing bubbles to increase user awareness. We will show that, while their methods differ, the benchmarks they use to break the filter bubble can be the same. We will also show that, a design can include criteria from multiple democracy conceptions that we discussed in the previous section. 
4.3.1. LIBERAL / USER AUTONOMY ENHANCING 
As we have stated in Section 2.1, in the liberal view of democracy, filter bubbles can be seen as a form of market failure that diminishes user control and hence autonomy, hide available options and coerce people in such a way that they cannot get what they want. Users will not get the search results they were looking for, or do not receive the updates from friends they want to in a social networking platform. Designers that take this view will develop tools that aim to promote awareness of filter bubbles and attempt to give
66 4. DEMOCRACY, FILTER BUBBLE AND DESIGN   
Figure 4.1: Scoopinion, a browser add-on that displays user’s 4 
news consumption habits. Larger circles are news outlets that the user consumed the most items 
Figure 4.2: Balancer [239] is a browser add-on that shows a user his bias. In this picture the user is biased towards reading from liberal news outlets 
users some sense of control. User satisfaction and awareness of options and choice seem to be the most common goals. As we will show in this subsection, this view of the filter bubble can be realized by giving users the control over the filters, increasing awareness of their own biases or increasing the awareness of the presence of filters that are imple mented in common web services. 
Munson et al. [2013] developed a browser tool called Balancer, that tracks user’s read ing activities and shows his reading behavior and bias, in order to increase awareness (See Also Figure 4.1). Munson et al. argue that, while many people agree that reading a diverse set of news is good, many do not realize how skewed their own reading be havior is. Balancer therefore shows an approximate histogram of the user’s liberal and conservative pages, with the hope that the given feedback will nudge the user to make his reading behavior more balanced. Munson et al. [2013] found that very low number of users changed their reading habits (conservatives consuming more liberal items and lib erals more conservative). The majority of the users did not change their reading habits at all. While Balancer aims for users to reflect their preferences and on the long-term in crease the epistemic quality of the incoming information, the primary goal is to increase user-awareness. Hence this tool belongs to the user autonomy enhancing technologies that are motivated by a liberal conception of democracy. 
Scoopinion 1is a browser add-on that tracks news sites and the type of stories one reads while using the browser. Scoopinion (See Figure 4.2) provides a visual summary of one’s reading habits by displaying user’s media fingerprint. The tool also personalizes recommended stories based upon user’s reading habits, but by displaying the media fin gerprint, it assumes that the user will choose to read more diversely. It works with a white-list of news sites and does not make diverse recommendations. It provides a visu alization of user’s information consumption habit to increase his autonomy, but it has no clear goals such as tolerance or better information quality. Again this fits a liberal conception of democracy and prioritizes the value of choice autonomy. 
1https://www.scoopinion.com/
4.3. SOFTWARE DESIGN TO COMBAT FILTER BUBBLES 
67 
4 (a) 
(b) 
Figure 4.3: Bobble [365] displays a user Google search results that only he received (in yellow) and results that he missed but others have received (in red) 
Xing et al [2014] developed a browser add-on called Bobble that allows the user to compare his Google search results with other profiles worldwide. The tool (See Figure 4.3) uses hundreds of nodes to distribute a user’s Google search queries worldwide each time the user performs a Google search. For example, when a user performs a Google search with keyword “Obamacare", this search keyword is distributed to 40+ worldwide Bobble clients that perform the same Google search and return corresponding search returns. The user can then see which results are displayed on his browser, but not on others, and vice versa. It is a tool for users to get an idea of the extent of personalization taking place. The tool to increase user’s awareness of Google’s filters. However, it does not aim to increase deliberation or provide challenging information by its design. 
Nagulendra and Vassileva [2014] developed a visualization design to display to users their filter bubbles (Figure 4.4). The tool helps users understand how information fil tering works in an online peer-to-peer social network. The tool shows the user which categories and friends are in their bubble and which ones are not. Further, it allows
68 4. DEMOCRACY, FILTER BUBBLE AND DESIGN 4 Figure 4.4: Nagulendra and Vassileva’s software [2014] allows users to control their filter bubbles. 
them to control the algorithm by manipulating the visualization to “escape” the bubble, namely adding/removing friends on a certain topic to the filters. The tool aims to maxi mize user control over his filter bubble, increase awareness of the filter bubble, promote understandability of the filtering mechanism and ultimately increase user satisfaction. It, however, does not make an attempt to expose users into challenging information. If the user wants to remain in a bubble, the tool will allow him to do that. Also in this case, a liberal notion of democracy with an emphasis on user autonomy is at the background of the development of this tool. 
4.3.2. DELIBERATIVE / ENHANCING EPISTEMIC QUALITY OF INFORMATION As we have mentioned in Section 2.2, filter bubbles can be seen as a problem, not be cause they prevent users getting what they want, but because they diminish the quality of the public discussion. Deliberative democracy assumes that users are, or should be, exposed to diverse viewpoints, so that they can discover disagreements, truths, perspec-
4.3. SOFTWARE DESIGN TO COMBAT FILTER BUBBLES 
69 
tives and finally make better decisions. Polarized users or users exposed to low quality (but agreeable and relevant) information will have bad consequences. In order to in crease epistemic quality of information, the wide range of opinions and perspectives on a particular topic may be made more visible and users can compare their opinions with others, even if they are opposing their own views. In the end, respect, legitimacy and consensus can be reached. In this subsection, we will list some of the tools that allow users to discover different viewpoints by visualization, showing pro/con arguments for a controversial topic, nudging them to listen to others, or by diversifying search results by modifying them for political search queries. 
Microsoft search engine Bing studied the effect of used language for nudging Bing Search engine users [368]. In this study, a sample of 179,195 people who used news re lated queries were selected and then their political behavior and their link click pattern 4 
were observed. Researchers found that, while 81% (76%) of Republicans (Democrats) click on items from one of the most polarized outlets of their own view, they rarely clicked on polarized outlets of the other side (4% and 6% respectively), suggesting a filter bub ble in action. The researchers then modified the Bing search engine’s results page. They matched Democratic to Republican-leaning queries on the same topic manually (e.g., obamacare and affordable health care). They then modified the results for the queries for a subset of people who issued them (treatment group), resulting in a diversified set of results: the results contained items from both republican and liberal sources, regardless of what the user has searched for. This did not increase the number of clicks on items from the opposing political news outlets. However, when the authors chose websites that use a language similar to the user’s own language, they observed a change of 25% toward the center. The authors thus conclude that when the language model of a document is closer to an individual’s language model, it has a higher chance of being read despite it describing an opposite viewpoint. The researchers aimed for “increasing exposure to varied political opinions with a goal of improving (and enhancing) civil discourse” [368]. 
Considerit [111, 188] is a deliberation (pro/con) tool that is developed with the aims of (1) helping people learn about political topics and possible tradeoffs between differ ent opinions (2) nudging them toward reflective consideration of other voters’ thoughts (3) enable users to see how others consider tradeoffs. ConsiderIt (Figure 4.5) provides an interface where users can create pro/con lists by including existing arguments others have contributed, to contribute new points themselves, and to use the results of these personal deliberations to expose salient points by summarizing their stance rather than a yes/no vote. Users can see ranked lists of items that were popular full opposers, firm opposers, slight opposers, neutrals, slight supporters, firm supporters and full support ers. In a pilot study called “The Living Voters Guide” (LVG), the system was put into test ing during 2010 Washington state elections that had certain proposals on areas of tax, sale of alcohol, candy or bottled water, state debt, bail and other political topics. In LVG, 8823 unique visitors browsed the site and 468 people submitted a position on at least one item. In a small survey of 7 users, 46.3% of them have reported that they have actu ally changed their stances on at least one measure and 56% of them saying they switched from support to oppose or vice versa. 32% of them have reported that they moderated their stances and 12% saying they strengthened them [188]. 
OpinionSpace [102] plots on a two-dimensional map the individual comments in a
70 4. DEMOCRACY, FILTER BUBBLE AND DESIGN 4 
Figure 4.5: Considerit [111, 188] helps people learn about political topics and possible tradeoffs between dif ferent opinions. 
web forum, based on the commenters’ responses to a short value-based questionnaire. By navigating this space, readers are better able to seek out a diversity of comments as well as prime themselves for engaging the perspective of someone with different values (Figure 4.6). When users interrogate an individual comment, they are prompted to rate comments for how much they agree with and respect it. The size of the comment’s dot on the map then grows when people with different values than the speaker respect and/or agree with it, facilitating users in seeking out comments that resonate widely. 
Reflect [187] modifies the comments of webpages in order to encourage listening and perspective taking. It adds a listening box next to every comment, where other users are encouraged to succinctly restate the points that the commenter is making, even if there is disagreement (Figure 4.7). This is a nudge to listen to other users. Other readers can afterwards read the original comment and other listeners’ interpretations of what was said, supporting broader understanding of the discussion. This way, those who not have to “like” or “recommend" the comment to recognize or appreciate the speaker. By nudging towards listening and reflecting, an empathetic and constructive normative en vironment is formed, where not only those who speak and reflect are positively affected, but those who read as well. In mid-September 2011, popular online discussion platform Slashdot enabled Reflect on four stories. During the trial, 734 reflections were written by 247 discussants, an average of 1.0 reflection per comment. While flaming and pure replies were present (31%), the majority of the reflections were neutral, different neutral interpretations or meta observations. The tool also allowed the community to rate re flections, making certain reflections under a threshold invisible. After users downvoted flaming or cheeky replies on those reflections, almost 80% of all the visible reflections were neutral reflections.
4.4. DISCUSSION 
71 
4 
Figure 4.6: Opinionspace [102] allows users to browse a diverse set of ideas, see responses from like-minded participants or responses from participants who differ in opinion. 
Rbutr2is a community driven Chrome add-on, that informs a user when the web page they are viewing has been disputed, rebutted or contradicted elsewhere on the In ternet (Figure 4.8). Users can add opposing viewpoints for an item, so that future users will see that an opposing viewpoint exists for the item they are reading. Rbutr aims to in crease information quality and informed opinions by promoting fact and logic-checking. 
There are other tools and studies that aim to increase epistemic quality of informa tion. Liao and Fu [2013, 2014] studied the effect of perceived threat, the level of topic involvement, and the effect of expertise and position indicators. Munson and Resnick [2010] studied the effect of nudging by sorting or highlighting agreeable news items and experimenting with the ratio of challenging and agreeable news items. Newscube [258, 259] is a tool that detects different aspects of a news using keyword analysis, and displays users news items with different perspectives in order to decrease media bias. Hypothes.is3is a community peer-review tool that allows the users to highlight text and add comments and sentence-level critic. Political Blend [88] is a mobile application that matches people with different political views and nudges them to have a cup of coffee face to face and discuss politics. 
Table 5.1 below summarizes our analysis of the studied tools. 
4.4. DISCUSSION 
One of the key finding of our analysis is that the benchmarks specified by agonistic and contestatory models of democracy are completely missing in all of the tools that aim to fight the filter bubble. While it is possible to come across critical voices, disadvan taged views or contestation using tools such as OpinionSpace or ConsiderIt, it is also highly likely that these voices and views get lost among the “popular” items, which are 
2http://rbutr.com/ 
3https://hypothes.is/
72 4. DEMOCRACY, FILTER BUBBLE AND DESIGN 
Model Examples Design Criteria (Bench marks) 
Liberal 
	Balancer 
Scoopinion 
Bobble 
Nagulendra and Vas sileva’s tool
	Allow users to be aware oftheir own (and the plat form’s) biases 
Allow users to understand biases 
Allow the user to con trol incoming information and filters
	Deliberative 
	Bing Study 
Considerit 
OpinionSpace 
Rbutr, Newscube 
Political Blend
	Discover diverse facts,perspectives and dis agreements 
Reflection on own (and others’) arguments 
Aim for informed debate with epistemic justifica tions 
Increase the epistemic quality of information
	



4 
Table 4.2: Tools that are developed to combat filter bubbles, the benchmarks they use and the models they belong to
4.4. DISCUSSION 
73 
4 
Figure 4.7: Reflect [187] nudges users to listen to each other by making them restate the points that the com menter is making, even if there is disagreement. 
of interest for the majority of the audience. However, as McQuail and van Cuilenburg [1983] have argued, media should not only proportionally reflect differences in politics, religion, culture and social conditions, but provide equal access to their channels for all people and all ideas in society. If the population preferences were uniformly distributed over society, then satisfying the first condition (reflection) would also satisfy the second condition (equal access). However, this is seldom the case [335]. Often population pref erences tend toward the middle and to the mainstream. In such cases, the media will not satisfy the openness norm, and the view of minorities will not reach a larger pub lic. This is undesirable, because social change usually begins with minority views and movements [335]. 
In modern democracies, some citizens are able to buy sufficient media time to dom inate public discussion, while others are excluded. If the political outcomes result from an exclusive process, where those with greater power or wealth are able to dominate the process, then from the point of view of democratic norms that outcome is illegitimate. However, even if people are formally included in the democratic process, inclusion is sues arise if they are not taken seriously or treated with respect. The dominant party may find their arguments not worthy enough for consideration. Then, people, while